{
  
    
        "post0": {
            "title": "Luigi ETL",
            "content": "About . Luigi is a python ETL framework built by Spotify. I use pandas in my day-to-day job and have created numerous pipeline tasks to move, transform, and analyze data across my organization. I thought Luigi would be a great addition to help manage these pipelines, but after reading their getting started documentation, it left me scratching my head. . If you are reading this, then I assume the docs have you confused as well, and hopefully, my post below can provide you with a bit more clarity. The post assumes you have already read the docs. If you haven&#39;t, please read that first before continuing. . Task Execution . Typical ETL Execution: . Task A $ longrightarrow$ Task B $ longrightarrow$ Task C . Luigi ETL Execution: . Task A $ longleftarrow$ Task B $ longleftarrow$ Task C . The most important thing to understand about Luigi is that it executes the ETL backward (recursively). It checks first to see if the current task (Task C) is completed. If not, it will then move backward to check if the previous task is completed (Task B). Once it finds the first completed task, it will then begin to execute the Tasks moving forward again. . This approach can save you a lot of time in your ETL. The reason is that you won&#39;t re-run completed tasks. However, this makes it a bit trickier to implement because you can find yourself in a situation where Task B or Task C will always return that they are complete, and Task A will never run. . How are Tasks linked? . Task A $ longleftarrow$ Task B $ longleftarrow$ Task C . Except for External Tasks, most other tasks are dependent on another Luigi Task. The way you define this dependency is by defining a requires() method in the Task Class and defining those dependent tasks(s). If the task is complete, it won&#39;t bother to check the dependent task(s). . What defines a completed Task? . Luigi considers a Task completed when the Task output exists. So if Task A outputs task_a.csv and it exists, then Task A will be considered complete. What the getting started docs fail to mention is that in reality, Task A is complete when Task A&#39;s method complete() returns True. The complete() method default behavior is to check if the output exists. We can override this behavior, and I would probably bet most do that have deployed Luigi into production. . Coding Demonstration of Task Execution . Task A $ longleftarrow$ Task B . The below code is an example of how to set up a Luigi Task. We have two classes, Task_A and Task_B, where Task_B is dependent on Task_A. I&#39;ve provided comments in the output to help visualize the order of events that take place when the Tasks run. . There are few things to note about the code: GlobalParam is a helper class to provide a global variable so I can count the execution events i.e. 1: complete () ... I replaced the Luigi complete() method with a similar method that checks if the output file exists so we could see the method executed in the print statements. . MockTarget creates an in-memory file object that we can write to and check if it exists. | . import luigi import pandas as pd from luigi.mock import MockTarget class GlobalParams(luigi.Config): count = luigi.IntParameter(default=1) class Task_A(luigi.Task): def output(self): return MockTarget(&quot;Task_A&quot;) def run(self): print(f&quot;{g.count}: run() {self.__class__.__name__} has no prior Task dependency. It is now running to complete the task&quot;) g.count += 1 out = self.output().open(&quot;w&quot;) out.write(&#39;complete&#39;) out.close() def complete(self): print(f&#39;{g.count}: complete() Checking to see if {self.__class__.__name__} has been completed&#39;) g.count += 1 return self.output().exists() class Task_B(luigi.Task): def requires(self): print(f&#39;{g.count}: requires() {self.__class__.__name__} is not completed, checking to see if previous tasks are required and completed&#39;) g.count += 1 return Task_A() def output(self): return MockTarget(&quot;Task_B&quot;) def run(self): print(f&#39;{g.count}: run() All previous tasks are completed and {self.__class__.__name__} is running to complete the task&#39;) g.count += 1 out = self.output().open(&quot;w&quot;) out.write(&#39;complete&#39;) out.close() print(f&#39;{g.count}: All Tasks are completed&#39;) def complete(self): print(f&#39;{g.count}: complete() Checking to see if {self.__class__.__name__} has been completed&#39;) g.count += 1 if self.output().exists(): print(f&#39;{g.count}: All Tasks are completed&#39;) return self.output().exists() g = GlobalParams() luigi.build([Task_B()], local_scheduler=True) . DEBUG: Checking if Task_B() is complete DEBUG: Checking if Task_A() is complete INFO: Informed scheduler that task Task_B__99914b932b has status PENDING INFO: Informed scheduler that task Task_A__99914b932b has status PENDING INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 2 INFO: [pid 2456] Worker Worker(salt=998663148, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running Task_A() INFO: [pid 2456] Worker Worker(salt=998663148, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done Task_A() DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task Task_A__99914b932b has status DONE DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 1 INFO: [pid 2456] Worker Worker(salt=998663148, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running Task_B() INFO: [pid 2456] Worker Worker(salt=998663148, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done Task_B() DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task Task_B__99914b932b has status DONE DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=998663148, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 2 tasks of which: * 2 ran successfully: - 1 Task_A() - 1 Task_B() This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . 1: complete() Checking to see if Task_B has been completed 2: requires() Task_B is not completed, checking to see if previous tasks are required and completed 3: complete() Checking to see if Task_A has been completed 4: run() Task_A has no prior Task dependency. It is now running to complete the task 5: requires() Task_B is not completed, checking to see if previous tasks are required and completed 6: complete() Checking to see if Task_A has been completed 7: run() All previous tasks are completed and Task_B is running to complete the task 8: All Tasks are completed . True . When we run the Task a second-time, note how Task_A is not referenced. Luigi checked to see if Task_B was complete and stopped the execution since it returned True. That means, that if some file upstream is updated and needed to be transformed by Task_Ait would not occur since Luigi would always stop at Task_B. . g.count=1 luigi.build([Task_B()], local_scheduler=True) . DEBUG: Checking if Task_B() is complete INFO: Informed scheduler that task Task_B__99914b932b has status DONE INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=177125647, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 1 tasks of which: * 1 complete ones were encountered: - 1 Task_B() Did not run any tasks This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . 1: complete() Checking to see if Task_B has been completed 2: All Tasks are completed . True . Luigi Paramaters . Words $ longleftarrow$ Count . Parameters are Luigi&#39;s intended way to make sure tasks get updated based on some frequency to make sure they don&#39;t get stuck in a &quot;complete&quot; status. Luigi offers there own Parameter object that is mostly intended to act as a constructor when executing tasks from the command line. . Below we have created two new Tasks, Words and Count. Each task takes a date as a parameter and appends the date to the file name output. You&#39;ll also notice I removed the complete() method. This means it will default to the original method that also checks if the output target exists more robustly. . import datetime from pathlib import Path OUTPUT_PATH = Path(&#39;output&#39;) class Words(luigi.Task): date = luigi.DateParameter(default=datetime.date.today()) def output(self): return luigi.LocalTarget(OUTPUT_PATH/f&#39;words_{self.date}.csv&#39;) def run(self): words = [&#39;apple&#39;,&#39;banana&#39;,&#39;grapefruit&#39;] df = pd.DataFrame(dict(words=words)) df.to_csv(self.output().path, index=False) class Count(luigi.Task): date = luigi.DateParameter(default=datetime.date.today()) def requires(self): # Passing the luigi paramater back to upstream task return Words(self.date) def output(self): return luigi.LocalTarget(OUTPUT_PATH/f&#39;count_{self.date}.csv&#39;) def run(self): df = pd.read_csv(self.input().path) df[&#39;letter_count&#39;] = df.words.map(len) df.to_csv(self.output().path, index=False) luigi.build([Count()], local_scheduler=True) . DEBUG: Checking if Count(date=2020-10-26) is complete DEBUG: Checking if Words(date=2020-10-26) is complete INFO: Informed scheduler that task Count_2020_10_26_424115e443 has status PENDING INFO: Informed scheduler that task Words_2020_10_26_424115e443 has status PENDING INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 2 INFO: [pid 2456] Worker Worker(salt=660151019, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running Words(date=2020-10-26) INFO: [pid 2456] Worker Worker(salt=660151019, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done Words(date=2020-10-26) DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task Words_2020_10_26_424115e443 has status DONE DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 1 INFO: [pid 2456] Worker Worker(salt=660151019, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running Count(date=2020-10-26) INFO: [pid 2456] Worker Worker(salt=660151019, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done Count(date=2020-10-26) DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task Count_2020_10_26_424115e443 has status DONE DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=660151019, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 2 tasks of which: * 2 ran successfully: - 1 Count(date=2020-10-26) - 1 Words(date=2020-10-26) This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . True . //Directory Tree: output/ ├── count_2020-10-26.csv └── words_2020-10-26.csv . Above, our Tasks ran successfully and saved the outputs in our output directory. So what happens if we were to run it a second time? . luigi.build([Count()], local_scheduler=True) . DEBUG: Checking if Count(date=2020-10-26) is complete INFO: Informed scheduler that task Count_2020_10_26_424115e443 has status DONE INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=832748578, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 1 tasks of which: * 1 complete ones were encountered: - 1 Count(date=2020-10-26) Did not run any tasks This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . True . //Directory Tree: output/ ├── count_2020-10-26.csv └── words_2020-10-26.csv . As you can see, nothing happened since the Count task encountered an output that already existed with the same name. Below we&#39;ll provide a different date to the Count task. . luigi.build([Count(date=pd.to_datetime(&#39;10/25/2021&#39;))], local_scheduler=True) . DEBUG: Checking if Count(date=2021-10-25) is complete DEBUG: Checking if Words(date=2021-10-25) is complete INFO: Informed scheduler that task Count_2021_10_25_8a7563aba6 has status PENDING INFO: Informed scheduler that task Words_2021_10_25_8a7563aba6 has status PENDING INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 2 INFO: [pid 2456] Worker Worker(salt=766530749, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running Words(date=2021-10-25) INFO: [pid 2456] Worker Worker(salt=766530749, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done Words(date=2021-10-25) DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task Words_2021_10_25_8a7563aba6 has status DONE DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 1 INFO: [pid 2456] Worker Worker(salt=766530749, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running Count(date=2021-10-25) INFO: [pid 2456] Worker Worker(salt=766530749, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done Count(date=2021-10-25) DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task Count_2021_10_25_8a7563aba6 has status DONE DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=766530749, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 2 tasks of which: * 2 ran successfully: - 1 Count(date=2021-10-25) - 1 Words(date=2021-10-25) This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . True . //Directory Tree: output/ ├── count_2020-10-26.csv ├── count_2021-10-25.csv ├── words_2020-10-26.csv └── words_2021-10-25.csv . External Tasks . If your pipeline starts with some dependency from an External Task, you can utilize the ExternalTask object. The External Task is the same as the Task object except it doesn&#39;t have a run() method. . External Task is useful because it allows for your task to gracefully end a job if the external source criteria are not met. . class Words(luigi.ExternalTask): def output(self): return luigi.LocalTarget(OUTPUT_PATH/f&#39;words.csv&#39;) luigi.build([Words()], local_scheduler=True) . DEBUG: Checking if Words() is complete WARNING: Data for Words() does not exist (yet?). The task is an external data dependency, so it cannot be run from this luigi process. INFO: Informed scheduler that task Words__99914b932b has status PENDING INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=806349904, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 1 tasks of which: * 1 were left pending, among these: * 1 were missing external dependencies: - 1 Words() Did not run any tasks This progress looks :| because there were missing external dependencies ===== Luigi Execution Summary ===== . True . Above, the Words external task did not run because words.csv, the external dependency, was missing. . OUTPUT_PATH = Path(&#39;output&#39;) words = [&#39;apple&#39;,&#39;banana&#39;,&#39;grapefruit&#39;] df = pd.DataFrame(dict(words=words)) df.to_csv(OUTPUT_PATH/&#39;words.csv&#39;, index=False) . Now that we created words.csv our external task will return as completed and pass its output to the next Task if it exists. . luigi.build([Words()], local_scheduler=True) . DEBUG: Checking if Words() is complete INFO: Informed scheduler that task Words__99914b932b has status DONE INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=781673351, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 1 tasks of which: * 1 complete ones were encountered: - 1 Words() Did not run any tasks This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . True . Alternate Complete Method . As I mentioned earlier, by default Luigi determines if a Task is complete by checking if the output exists. However, there is a common use case in pipeline workflows where Tasks should be run when a file is updated. Since Luigi only checks the output name it will determine that a Task is completed no matter how many times a file gets updated. . However, we can override this method by overriding the complete() method in the Task object by defining your criteria. It needs to return False if the task is not complete and True if the task is complete. . Below we will create our own complete() function that will update all tasks where their dependent task output files have been updated. It will first check to see if the dependency tasks have been completed, then check to see if the modified time of the output of the current task is greater than the modified time of the prior task&#39;s output. . import os import time class Words(luigi.ExternalTask): def output(self): return luigi.LocalTarget(OUTPUT_PATH/&#39;words.csv&#39;) class CountLetters(luigi.Task): def requires(self): return Words() # Custom Complete Method def complete(self): if not self.output().exists(): print(&#39;//Count Letters: No Output File&#39;) return False input_mtime = time.ctime(os.path.getmtime(self.input().path)) output_mtime = time.ctime(os.path.getmtime(self.output().path)) if output_mtime &lt; input_mtime: print(&#39;//Count Letters: File Out of Date&#39;) return False print(&#39;//Count Letters: Task is Complete&#39;) return True def output(self): return luigi.LocalTarget(OUTPUT_PATH/&#39;count_letters.csv&#39;) def run(self): df = pd.read_csv(self.input().path) df[&#39;letter_count&#39;] = df.words.map(len) df.to_csv(self.output().path, index=False) . luigi.build([CountLetters()], local_scheduler=True) . DEBUG: Checking if CountLetters() is complete DEBUG: Checking if Words() is complete INFO: Informed scheduler that task CountLetters__99914b932b has status PENDING INFO: Informed scheduler that task Words__99914b932b has status DONE INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 1 INFO: [pid 2456] Worker Worker(salt=437114638, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running CountLetters() INFO: [pid 2456] Worker Worker(salt=437114638, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done CountLetters() DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task CountLetters__99914b932b has status DONE DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=437114638, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 2 tasks of which: * 1 complete ones were encountered: - 1 Words() * 1 ran successfully: - 1 CountLetters() This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . //Count Letters: No Output File . True . //Directory Tree: output/ ├── count_letters.csv └── words.csv . When we run the task again it doesn&#39;t run any tasks because words.csv exists and its modified time (mtime) is less than count_letters.csv modified time. . luigi.build([CountLetters()], local_scheduler=True) . DEBUG: Checking if CountLetters() is complete INFO: Informed scheduler that task CountLetters__99914b932b has status DONE INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=438850535, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 1 tasks of which: * 1 complete ones were encountered: - 1 CountLetters() Did not run any tasks This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . //Count Letters: Task is Complete . True . We will now update count_letters.csv to include a few more words and watch our Luigi run the Task because of the updated modification times on the files. . words = [&#39;apple&#39;,&#39;banana&#39;,&#39;grapefruit&#39;, &#39;cherry&#39;, &#39;orange&#39;] df = pd.DataFrame(dict(words=words)) df.to_csv(OUTPUT_PATH/&#39;words.csv&#39;, index=False) luigi.build([CountLetters()], local_scheduler=True) . DEBUG: Checking if CountLetters() is complete DEBUG: Checking if Words() is complete INFO: Informed scheduler that task CountLetters__99914b932b has status PENDING INFO: Informed scheduler that task Words__99914b932b has status DONE INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 1 INFO: [pid 2456] Worker Worker(salt=695141065, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running CountLetters() INFO: [pid 2456] Worker Worker(salt=695141065, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done CountLetters() DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task CountLetters__99914b932b has status DONE DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=695141065, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 2 tasks of which: * 1 complete ones were encountered: - 1 Words() * 1 ran successfully: - 1 CountLetters() This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . //Count Letters: File Out of Date . True . SQL Tasks . Words $ longleftarrow$ Count $ longleftarrow$ StoreSQL $ longleftarrow$ PrintSQL . SQL is a common step in many pipelines but the Luigi getting started docs barely cover the topic. In this section we will create two new tasks. The first task, StoreSql, will take the ouput from CountLetters and store it in a SQLite database. The second task, PrintSQL, will then read from out database and print both tables that Luigi created. . The big difference between LocalTarget is that SQLAlchemyTarget creates and updates a &quot;Marker Table&quot; to keep track of whether a task is complete or not. You provide the Marker Table with a update_id and Luigi will check if it exists before running the Task. . Below I&#39;ve provided the code for our StoreSQL and PrintSQL tasks. There are a couple of things worth noting. . We have overridden the complete() method to check if the prior task CountLetters has been completed and if the StoreSQL task output exists. If either returns False the Task will run. | We are creating an SQLite database called my.db. | self.output().touch() is what marks the Task as complete and creates/updates the Marker Table | PrintSQL is complete method is set to False so that it always runs for demonstration purposes. | . //Directory Tree: output/ ├── count_letters.csv └── words.csv . from luigi.contrib import sqla from sqlalchemy import create_engine OUTPUT_PATH = Path(&#39;output&#39;) connection_string = f&quot;sqlite:///{OUTPUT_PATH}/my.db&quot; outputs = [] class StoreSQL(luigi.Task): connection_string = luigi.Parameter() target_table = luigi.Parameter() @property def update_id(self): mtime = os.path.getmtime(self.input().path) mtime = datetime.datetime.fromtimestamp(mtime).strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) return mtime + &#39;_&#39; + self.target_table def complete(self): if not self.output().exists(): return False if not self.requires().complete(): return False return True def requires(self): return CountLetters() def output(self): return sqla.SQLAlchemyTarget( connection_string=self.connection_string, target_table=self.target_table, update_id=self.update_id ) def run(self): self.requires().complete() con = self.output().engine df = pd.read_csv(self.input().path) df.to_sql(name=self.target_table, con=con, if_exists=&#39;replace&#39;) # Update Marker Table self.output().touch() class PrintSQL(luigi.Task): connection_string = luigi.Parameter() target_table = luigi.Parameter() def requires(self): return StoreSQL(self.connection_string, self.target_table) def complete(self): return False def output(self): pass def run(self): input = self.input() con = input.engine table = input.target_table print(&#39;// Letter Count Table&#39;) print(pd.read_sql(sql=table, con=con), end=&#39; n n&#39;) print(&#39;// Marker Table&#39;) print(pd.read_sql(sql=&#39;table_updates&#39;, con=con)) . luigi.build([PrintSQL(connection_string, target_table=&#39;letter_count&#39;)], local_scheduler=True) . DEBUG: Checking if PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) is complete DEBUG: Checking if StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) is complete INFO: Informed scheduler that task PrintSQL_sqlite____output_letter_count_4c5210e673 has status PENDING DEBUG: Checking if CountLetters() is complete INFO: Informed scheduler that task StoreSQL_sqlite____output_letter_count_4c5210e673 has status PENDING INFO: Informed scheduler that task CountLetters__99914b932b has status DONE INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 2 INFO: [pid 2456] Worker Worker(salt=382729348, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) INFO: [pid 2456] Worker Worker(salt=382729348, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task StoreSQL_sqlite____output_letter_count_4c5210e673 has status DONE DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 1 INFO: [pid 2456] Worker Worker(salt=382729348, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) INFO: [pid 2456] Worker Worker(salt=382729348, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task PrintSQL_sqlite____output_letter_count_4c5210e673 has status DONE DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=382729348, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 3 tasks of which: * 1 complete ones were encountered: - 1 CountLetters() * 2 ran successfully: - 1 PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) - 1 StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . // Letter Count Table index words letter_count 0 0 apple 5 1 1 banana 6 2 2 grapefruit 10 3 3 cherry 6 4 4 orange 6 // Marker Table update_id target_table inserted 0 2020-10-26 17:09:56_letter_count letter_count 2020-10-26 17:10:30.763616 . True . paths = DisplayablePath.make_tree(OUTPUT_PATH) print(&#39;Output Directory Tree:&#39;) for path in paths: print(path.displayable()) . Output Directory Tree: output/ ├── count_letters.csv ├── my.db └── words.csv . As you can see from above, our SQL Task has updated two tables and printed out the table results. If you look at the Marker table update_id column you&#39;ll notice it is the concatenation of our count_letters.csv mtime and target table name. . Now let&#39;s update our words.csv and see what happens when we run the task again. . words = [&#39;apple&#39;,&#39;banana&#39;,&#39;grapefruit&#39;, &#39;cherry&#39;, &#39;orange&#39;, &#39;peach&#39;, &#39;strawberry&#39;] df = pd.DataFrame(dict(words=words)) df.to_csv(OUTPUT_PATH/&#39;words.csv&#39;, index=False) luigi.build([PrintSQL(connection_string, target_table=&#39;letter_count&#39;)], local_scheduler=True) . DEBUG: Checking if PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) is complete DEBUG: Checking if StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) is complete INFO: Informed scheduler that task PrintSQL_sqlite____output_letter_count_4c5210e673 has status PENDING DEBUG: Checking if CountLetters() is complete INFO: Informed scheduler that task StoreSQL_sqlite____output_letter_count_4c5210e673 has status PENDING DEBUG: Checking if Words() is complete INFO: Informed scheduler that task CountLetters__99914b932b has status PENDING INFO: Informed scheduler that task Words__99914b932b has status DONE INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 3 INFO: [pid 2456] Worker Worker(salt=051857807, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running CountLetters() INFO: [pid 2456] Worker Worker(salt=051857807, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done CountLetters() DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task CountLetters__99914b932b has status DONE DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 2 INFO: [pid 2456] Worker Worker(salt=051857807, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) INFO: [pid 2456] Worker Worker(salt=051857807, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task StoreSQL_sqlite____output_letter_count_4c5210e673 has status DONE DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 1 INFO: [pid 2456] Worker Worker(salt=051857807, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) INFO: [pid 2456] Worker Worker(salt=051857807, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task PrintSQL_sqlite____output_letter_count_4c5210e673 has status DONE DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=051857807, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 4 tasks of which: * 1 complete ones were encountered: - 1 Words() * 3 ran successfully: - 1 CountLetters() - 1 PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) - 1 StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . // Letter Count Table index words letter_count 0 0 apple 5 1 1 banana 6 2 2 grapefruit 10 3 3 cherry 6 4 4 orange 6 5 5 peach 5 6 6 strawberry 10 // Marker Table update_id target_table inserted 0 2020-10-26 17:09:56_letter_count letter_count 2020-10-26 17:10:30.763616 1 2020-10-26 17:10:34_letter_count letter_count 2020-10-26 17:10:34.734229 . True . As expected, our Letter Count table updated and the Marker Table&#39;s contains a new row to represent the task completing. . Let&#39;s run the task one more time without updating words.csv. . luigi.build([PrintSQL(connection_string, target_table=&#39;letter_count&#39;)], local_scheduler=True) . DEBUG: Checking if PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) is complete DEBUG: Checking if StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) is complete INFO: Informed scheduler that task PrintSQL_sqlite____output_letter_count_4c5210e673 has status PENDING INFO: Informed scheduler that task StoreSQL_sqlite____output_letter_count_4c5210e673 has status DONE INFO: Done scheduling tasks INFO: Running Worker with 1 processes DEBUG: Asking scheduler for work... DEBUG: Pending tasks: 1 INFO: [pid 2456] Worker Worker(salt=023539738, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) running PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) INFO: [pid 2456] Worker Worker(salt=023539738, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) done PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) DEBUG: 1 running tasks, waiting for next task to finish INFO: Informed scheduler that task PrintSQL_sqlite____output_letter_count_4c5210e673 has status DONE DEBUG: Asking scheduler for work... DEBUG: Done DEBUG: There are no more tasks to run at this time INFO: Worker Worker(salt=023539738, workers=1, host=DESKTOP-BCU4BGH, username=Mike, pid=2456) was stopped. Shutting down Keep-Alive thread INFO: ===== Luigi Execution Summary ===== Scheduled 2 tasks of which: * 1 complete ones were encountered: - 1 StoreSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) * 1 ran successfully: - 1 PrintSQL(connection_string=sqlite:///output/my.db, target_table=letter_count) This progress looks :) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== . // Letter Count Table index words letter_count 0 0 apple 5 1 1 banana 6 2 2 grapefruit 10 3 3 cherry 6 4 4 orange 6 5 5 peach 5 6 6 strawberry 10 // Marker Table update_id target_table inserted 0 2020-10-26 17:09:56_letter_count letter_count 2020-10-26 17:10:30.763616 1 2020-10-26 17:10:34_letter_count letter_count 2020-10-26 17:10:34.734229 . True . The only task that ran was the PrintSQL task, and our other task(s) didn&#39;t run. The Marker Table was also not updated. . Resources . https://stackoverflow.com/questions/40407936/mysql-targets-in-luigi-workflow/40423427#40423427 | https://stackoverflow.com/questions/40707004/using-luigi-to-update-postgres-table | https://stackoverflow.com/questions/28793832/can-luigi-rerun-tasks-when-the-task-dependencies-become-out-of-date | https://luigi.readthedocs.io/en/stable/_modules/luigi/contrib/sqla.html | https://stackoverflow.com/questions/9727673/list-directory-tree-structure-in-python | https://stackoverflow.com/questions/11349333/how-to-ignore-the-first-line-of-data-when-processing-csv-data | https://stackoverflow.com/questions/35918605/how-to-delete-a-table-in-sqlalchemy | https://stackoverflow.com/questions/11900553/sqlalchemy-table-already-exists | https://stackoverflow.com/questions/237079/how-to-get-file-creation-modification-date-times-in-python | https://stackoverflow.com/questions/48509083/how-to-make-a-parameter-available-to-all-luigi-tasks | .",
            "url": "https://gardnmi.github.io/blog/jupyter/pandas/luigi/python/2020/10/26/luigi-etl.html",
            "relUrl": "/jupyter/pandas/luigi/python/2020/10/26/luigi-etl.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Predicting Stock Prices with Prophet",
            "content": "Imports . import pandas as pd import numpy as np from fbprophet import Prophet import matplotlib.pyplot as plt from functools import reduce %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) plt.style.use(&#39;seaborn-deep&#39;) pd.options.display.float_format = &quot;{:,.2f}&quot;.format . For this project we will be importing the standard libraries for data anaysis with Python. We will also import Prophet and reduce from functools which will be used to help simulate our Forecasts. . The Data . stock_price = pd.read_csv(&#39;^GSPC.csv&#39;,parse_dates=[&#39;Date&#39;]) . stock_price.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 9885 entries, 0 to 9884 Data columns (total 7 columns): Date 9885 non-null datetime64[ns] Open 9885 non-null float64 High 9885 non-null float64 Low 9885 non-null float64 Close 9885 non-null float64 Adj Close 9885 non-null float64 Volume 9885 non-null int64 dtypes: datetime64[ns](1), float64(5), int64(1) memory usage: 540.7 KB . stock_price.describe() . Open High Low Close Adj Close Volume . count 9,885.00 | 9,885.00 | 9,885.00 | 9,885.00 | 9,885.00 | 9,885.00 | . mean 958.64 | 964.26 | 952.65 | 958.86 | 958.86 | 1,628,930,637.33 | . std 696.19 | 699.39 | 692.67 | 696.24 | 696.24 | 1,765,031,087.55 | . min 98.22 | 99.58 | 94.23 | 98.22 | 98.22 | 14,990,000.00 | . 25% 326.24 | 328.00 | 322.98 | 326.35 | 326.35 | 169,010,000.00 | . 50% 955.40 | 965.38 | 949.45 | 955.41 | 955.41 | 805,900,000.00 | . 75% 1,347.74 | 1,355.87 | 1,336.36 | 1,347.74 | 1,347.74 | 3,150,330,000.00 | . max 2,936.76 | 2,940.91 | 2,927.11 | 2,930.75 | 2,930.75 | 11,456,230,000.00 | . The data we are using is the historical S&amp;P500 prices dating back to 1980. You can find the data here. . Data Preparation . stock_price = stock_price[[&#39;Date&#39;,&#39;Adj Close&#39;]] . stock_price.columns = [&#39;ds&#39;, &#39;y&#39;] stock_price.head(10) . ds y . 0 1980-01-02 | 105.76 | . 1 1980-01-03 | 105.22 | . 2 1980-01-04 | 106.52 | . 3 1980-01-07 | 106.81 | . 4 1980-01-08 | 108.95 | . 5 1980-01-09 | 109.05 | . 6 1980-01-10 | 109.89 | . 7 1980-01-11 | 109.92 | . 8 1980-01-14 | 110.38 | . 9 1980-01-15 | 111.14 | . For prophet to work, we need to change the names of the &#39;Date&#39; and &#39;Adj Close&#39; columns to &#39;ds&#39; and &#39;y&#39;. The term &#39;y&#39; is typically used for the target column (what you are trying to predict) in most machine learning projects. . Prophet . stock_price.set_index(&#39;ds&#39;).y.plot(figsize=(12,6), grid=True); . Before we use Prophet to create a forecast let&#39;s visualize our data. It&#39;s always a good idea to create a few visualitions to gain a better understanding of the data you are working with. . model = Prophet() model.fit(stock_price) . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . &lt;fbprophet.forecaster.Prophet at 0x21216301c18&gt; . To activate the Prophet Model we simply call Prophet() and assign it to a variabl called model. Next fit our stock data to the model by calling the fit method. . future = model.make_future_dataframe(1095, freq=&#39;d&#39;) future_boolean = future[&#39;ds&#39;].map(lambda x : True if x.weekday() in range(0, 5) else False) future = future[future_boolean] future.tail() . ds . 10973 2022-03-07 | . 10974 2022-03-08 | . 10975 2022-03-09 | . 10976 2022-03-10 | . 10977 2022-03-11 | . To create a forecast with our model we need to create some futue dates. Prophet provides us with a helper function called make_future_dataframe. We pass in the number of future periods and frequency. Above we created a forecast for the next 1095 days or 3 years. . Since stocks can only be traded on weekdays we need to remove the weekends from our forecast dataframe. To do so we create a boolean expression where if a day does not equal 0 - 4 then return False. &quot;0 = Monday, 6=Saturday, etc..&quot; . We then pass the boolean expression to our dataframe with returns only True values. We now have a forecast dataframe comprised of the next 3 years of weekdays. . forecast = model.predict(future) . forecast.tail() . ds trend yhat_lower yhat_upper trend_lower trend_upper additive_terms additive_terms_lower additive_terms_upper weekly weekly_lower weekly_upper yearly yearly_lower yearly_upper multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper yhat . 10661 2022-03-07 | 3,398.27 | 3,043.92 | 3,765.33 | 3,050.21 | 3,757.76 | 0.46 | 0.46 | 0.46 | 0.90 | 0.90 | 0.90 | -0.44 | -0.44 | -0.44 | 0.00 | 0.00 | 0.00 | 3,398.73 | . 10662 2022-03-08 | 3,398.82 | 3,035.27 | 3,784.78 | 3,049.62 | 3,758.73 | 1.09 | 1.09 | 1.09 | 1.40 | 1.40 | 1.40 | -0.30 | -0.30 | -0.30 | 0.00 | 0.00 | 0.00 | 3,399.92 | . 10663 2022-03-09 | 3,399.38 | 3,044.68 | 3,769.69 | 3,049.04 | 3,759.69 | 1.25 | 1.25 | 1.25 | 1.37 | 1.37 | 1.37 | -0.12 | -0.12 | -0.12 | 0.00 | 0.00 | 0.00 | 3,400.63 | . 10664 2022-03-10 | 3,399.93 | 3,034.44 | 3,796.22 | 3,048.49 | 3,760.65 | 1.43 | 1.43 | 1.43 | 1.32 | 1.32 | 1.32 | 0.10 | 0.10 | 0.10 | 0.00 | 0.00 | 0.00 | 3,401.36 | . 10665 2022-03-11 | 3,400.49 | 3,040.25 | 3,798.08 | 3,047.96 | 3,761.62 | 1.48 | 1.48 | 1.48 | 1.11 | 1.11 | 1.11 | 0.37 | 0.37 | 0.37 | 0.00 | 0.00 | 0.00 | 3,401.97 | . To create the forecast we call predict from our model and pass in the future dataframe we created earlier. We return the results in a new dataframe called forecast. . When we inspect the forecast dataframe we see a bunch of new terms. The one we are most interested in is yhat which is our forecasted value. . model.plot(forecast); . model.plot_components(forecast); . All the new fields appear a bit daunting but fortunately Prophet comes with two handy visualization helpers, plot and plot_components. The plot functions creates a graph of our actuals and forecast and plot_components provides us a graph of our trend and seasonality. . stock_price_forecast = forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]] df = pd.merge(stock_price, stock_price_forecast, on=&#39;ds&#39;, how=&#39;right&#39;) df.set_index(&#39;ds&#39;).plot(figsize=(16,8), color=[&#39;royalblue&#39;, &quot;#34495e&quot;, &quot;#e74c3c&quot;, &quot;#e74c3c&quot;], grid=True); . The visualization helpers are just using the data in our forecast dataframe. We can actually recreate the same graphs. Above I recreated the plot graph. . Simulating Forecasts . While the 3 year forecast we created above is pretty cool we don&#39;t want to make any trading decisions on it without backtesting the performance and a trading strategy. . In this section we will simulate as if Prophet existed back in 1980 and we used it to creat a monthly forecast through 2019. We will then use this data in the following section to simulate how various trading strategies did vs if we just bought and held on to the stock. . stock_price[&#39;dayname&#39;] = stock_price[&#39;ds&#39;].dt.day_name() stock_price[&#39;month&#39;] = stock_price[&#39;ds&#39;].dt.month stock_price[&#39;year&#39;] = stock_price[&#39;ds&#39;].dt.year stock_price[&#39;month/year&#39;] = stock_price[&#39;month&#39;].map(str) + &#39;/&#39; + stock_price[&#39;year&#39;].map(str) stock_price = pd.merge(stock_price, stock_price[&#39;month/year&#39;].drop_duplicates().reset_index(drop=True).reset_index(), on=&#39;month/year&#39;, how=&#39;left&#39;) stock_price = stock_price.rename(columns={&#39;index&#39;:&#39;month/year_index&#39;}) . stock_price.tail() . ds y dayname month year month/year month/year_index . 9880 2019-03-08 | 2,743.07 | Friday | 3 | 2019 | 3/2019 | 470 | . 9881 2019-03-11 | 2,783.30 | Monday | 3 | 2019 | 3/2019 | 470 | . 9882 2019-03-12 | 2,791.52 | Tuesday | 3 | 2019 | 3/2019 | 470 | . 9883 2019-03-13 | 2,810.92 | Wednesday | 3 | 2019 | 3/2019 | 470 | . 9884 2019-03-14 | 2,808.48 | Thursday | 3 | 2019 | 3/2019 | 470 | . Before we simulate the monthly forecasts we need to add some columns to our stock_price dataframe we created in the beginning of this project to make it a bit easier to work with. We add month, year, month/year, and month/year_index. . loop_list = stock_price[&#39;month/year&#39;].unique().tolist() max_num = len(loop_list) - 1 forecast_frames = [] for num, item in enumerate(loop_list): if num == max_num: pass else: df = stock_price.set_index(&#39;ds&#39;)[ stock_price[stock_price[&#39;month/year&#39;] == loop_list[0]][&#39;ds&#39;].min(): stock_price[stock_price[&#39;month/year&#39;] == item][&#39;ds&#39;].max()] df = df.reset_index()[[&#39;ds&#39;, &#39;y&#39;]] model = Prophet() model.fit(df) future = stock_price[stock_price[&#39;month/year_index&#39;] == (num + 1)][[&#39;ds&#39;]] forecast = model.predict(future) forecast_frames.append(forecast) . stock_price_forecast = reduce(lambda top, bottom: pd.concat([top, bottom], sort=False), forecast_frames) stock_price_forecast = stock_price_forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]] stock_price_forecast.to_csv(&#39;stock_price_forecast.csv&#39;, index=False) . Above is a lot but essentially we are looping through each unique month/year in the stock_price and fitting the Prophet model with the stock data available to that period and then forecasting out one month ahead. We continue to do this until we hit the last unique month/year. Finally we combine these forecasts into a single dataframe called stock_price_forecast. I save the results as it take a while to run and in case I need to reset I can pull the csv file instead of running the model again. . stock_price_forecast = pd.read_csv(&#39;stock_price_forecast.csv&#39;, parse_dates=[&#39;ds&#39;]) . df = pd.merge(stock_price[[&#39;ds&#39;,&#39;y&#39;, &#39;month/year_index&#39;]], stock_price_forecast, on=&#39;ds&#39;) df[&#39;Percent Change&#39;] = df[&#39;y&#39;].pct_change() df.set_index(&#39;ds&#39;)[[&#39;y&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]].plot(figsize=(16,8), color=[&#39;royalblue&#39;, &quot;#34495e&quot;, &quot;#e74c3c&quot;, &quot;#e74c3c&quot;], grid=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2121698dc18&gt; . df.head() . ds y month/year_index yhat yhat_lower yhat_upper Percent Change . 0 1980-02-01 | 115.12 | 1 | 115.23 | 114.60 | 115.85 | nan | . 1 1980-02-04 | 114.37 | 1 | 115.74 | 115.12 | 116.46 | -0.01 | . 2 1980-02-05 | 114.66 | 1 | 116.05 | 115.38 | 116.75 | 0.00 | . 3 1980-02-06 | 115.72 | 1 | 116.79 | 116.15 | 117.50 | 0.01 | . 4 1980-02-07 | 116.28 | 1 | 116.50 | 115.79 | 117.23 | 0.00 | . Finally we combine our forecast with the actual prices and create a Percent Change column which will be used in our Trading Algorithms below. Lastly, I plot the forecasts with the actuals to see how well it did. As you can see there is a bit of a delay. It kind of behaves a lot like a moving average would. . Trading Algorithms . df[&#39;Hold&#39;] = (df[&#39;Percent Change&#39;] + 1).cumprod() df[&#39;Prophet&#39;] = ((df[&#39;yhat&#39;].shift(-1) &gt; df[&#39;yhat&#39;]).shift(1) * (df[&#39;Percent Change&#39;]) + 1).cumprod() df[&#39;Prophet Thresh&#39;] = ((df[&#39;y&#39;] &gt; df[&#39;yhat_lower&#39;]).shift(1)* (df[&#39;Percent Change&#39;]) + 1).cumprod() df[&#39;Seasonality&#39;] = ((~df[&#39;ds&#39;].dt.month.isin([8,9])).shift(1) * (df[&#39;Percent Change&#39;]) + 1).cumprod() . Above we create four initial trading algorithms: . Hold: Our bench mark. This is a buy and hold strategy. Meaning we buy the stock and hold on to it until the end time period. | Prophet: This strategy is to sell when our forecast indicates a down trend and buy back in when it iindicates an upward trend | Prophet Thresh: This strategy is to only sell when the stock price fall below our yhat_lower boundary. | Seasonality: This strategy is to exit the market in August and re-enter in Ocober. This was based on the seasonality chart from above. | . (df.dropna().set_index(&#39;ds&#39;)[[&#39;Hold&#39;, &#39;Prophet&#39;, &#39;Prophet Thresh&#39;,&#39;Seasonality&#39;]] * 1000).plot(figsize=(16,8), grid=True) print(f&quot;Hold = {df[&#39;Hold&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet = {df[&#39;Prophet&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Thresh = {df[&#39;Prophet Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Seasonality = {df[&#39;Seasonality&#39;].iloc[-1]*1000:,.0f}&quot;) . Hold = 24,396 Prophet = 13,366 Prophet Thresh = 17,087 Seasonality = 30,861 . Above we plot the results simulating an initial investment of $1,000 dollars. As you can see our Seasonality did best and our benchmark strategy of Hold did the second best. Both Prophet based strategies didn&#39;t do so well. Let&#39;s see if we can improve the Prophet Thresh buy optimizing the threshold. . performance = {} for x in np.linspace(.9,.99,10): y = ((df[&#39;y&#39;] &gt; df[&#39;yhat_lower&#39;]*x).shift(1)* (df[&#39;Percent Change&#39;]) + 1).cumprod() performance[x] = y best_yhat = pd.DataFrame(performance).max().idxmax() pd.DataFrame(performance).plot(figsize=(16,8), grid=True); f&#39;Best Yhat = {best_yhat:,.2f}&#39; . &#39;Best Yhat = 0.92&#39; . Above we loop through various percents of the thresh to find the optimal thresh. It appears the best threshhold is 92% of our current yhat_lower. . df[&#39;Optimized Prophet Thresh&#39;] = ((df[&#39;y&#39;] &gt; df[&#39;yhat_lower&#39;] * best_yhat).shift(1) * (df[&#39;Percent Change&#39;]) + 1).cumprod() . (df.dropna().set_index(&#39;ds&#39;)[[&#39;Hold&#39;, &#39;Prophet&#39;, &#39;Prophet Thresh&#39;, &#39;Seasonality&#39;, &#39;Optimized Prophet Thresh&#39;]] * 1000).plot(figsize=(16,8), grid=True) print(f&quot;Hold = {df[&#39;Hold&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet = {df[&#39;Prophet&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Thresh = {df[&#39;Prophet Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Seasonality = {df[&#39;Seasonality&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Optimized Prophet Thresh = {df[&#39;Optimized Prophet Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) . Hold = 24,396 Prophet = 13,366 Prophet Thresh = 17,087 Seasonality = 30,861 Optimized Prophet Thresh = 36,375 . Above we see our new Optimized Prophet Thresh is the best trading stratege. Unfortunately, both the Seasonaity and Optimized Prophet Thresh are both cheating since they are using data from the future that wouldn&#39;t be available at the time of our trade. We are going to need to create an Optimized Thresh for each curent point in time of our Forecast. . fcst_thresh = {} for num, index in enumerate(df[&#39;month/year_index&#39;].unique()): temp_df = df.set_index(&#39;ds&#39;)[ df[df[&#39;month/year_index&#39;] == df[&#39;month/year_index&#39;].unique()[0]][&#39;ds&#39;].min(): df[df[&#39;month/year_index&#39;] == index][&#39;ds&#39;].max()] performance = {} for thresh in np.linspace(0, .99, 100): percent = ((temp_df[&#39;y&#39;] &gt; temp_df[&#39;yhat_lower&#39;] * thresh).shift(1)* (temp_df[&#39;Percent Change&#39;]) + 1).cumprod() performance[thresh] = percent best_thresh = pd.DataFrame(performance).max().idxmax() if num == len(df[&#39;month/year_index&#39;].unique())-1: pass else: fcst_thresh[df[&#39;month/year_index&#39;].unique()[num+1]] = best_thresh . fcst_thresh = pd.DataFrame([fcst_thresh]).T.reset_index().rename(columns={&#39;index&#39;:&#39;month/year_index&#39;, 0:&#39;Fcst Thresh&#39;}) . fcst_thresh[&#39;Fcst Thresh&#39;].plot(figsize=(16,8), grid=True); . Above, like how we created our monthly forecast, we loop through the data and find the optimal thresh percent period to date for that current point in time. As you can see the % of the current thresh jumps around as we get further in to the periods (1/1/1980 - 3/18/2019). . df[&#39;yhat_optimized&#39;] = pd.merge(df, fcst_thresh, on=&#39;month/year_index&#39;, how=&#39;left&#39;)[&#39;Fcst Thresh&#39;].shift(1) * df[&#39;yhat_lower&#39;] . df[&#39;Prophet Fcst Thresh&#39;] = ((df[&#39;y&#39;] &gt; df[&#39;yhat_optimized&#39;]).shift(1)* (df[&#39;Percent Change&#39;]) + 1).cumprod() . (df.dropna().set_index(&#39;ds&#39;)[[&#39;Hold&#39;, &#39;Prophet&#39;, &#39;Prophet Thresh&#39;, &#39;Prophet Fcst Thresh&#39;]] * 1000).plot(figsize=(16,8), grid=True) print(f&quot;Hold = {df[&#39;Hold&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet = {df[&#39;Prophet&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Thresh = {df[&#39;Prophet Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) # print(f&quot;Seasonality = {df[&#39;Seasonality&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Fcst Thresh = {df[&#39;Prophet Fcst Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) . Hold = 24,396 Prophet = 13,366 Prophet Thresh = 17,087 Prophet Fcst Thresh = 20,620 . As we did before we create the new trading strategy and graph it. Unfortunately are results have gotten worse but we did do better then our initial Prophet Thresh. Instead of calculating the thresh using the full period to date let&#39;s try various rolling windows of time like you would see with a moving average(30, 60, 90, etc.). . rolling_thresh = {} for num, index in enumerate(df[&#39;month/year_index&#39;].unique()): rolling_performance = {} for roll in range(10, 400, 10): temp_df = df.set_index(&#39;ds&#39;)[ df[df[&#39;month/year_index&#39;] == index][&#39;ds&#39;].min() - pd.DateOffset(months=roll): df[df[&#39;month/year_index&#39;] == index][&#39;ds&#39;].max()] performance = {} for thresh in np.linspace(.0,.99, 100): percent = ((temp_df[&#39;y&#39;] &gt; temp_df[&#39;yhat_lower&#39;] * thresh).shift(1)* (temp_df[&#39;Percent Change&#39;]) + 1).cumprod() performance[thresh] = percent per_df = pd.DataFrame(performance) best_thresh = per_df.iloc[[-1]].max().idxmax() percents = per_df[best_thresh] rolling_performance[best_thresh] = percents per_df = pd.DataFrame(rolling_performance) best_rolling_thresh = per_df.iloc[[-1]].max().idxmax() if num == len(df[&#39;month/year_index&#39;].unique())-1: pass else: rolling_thresh[df[&#39;month/year_index&#39;].unique()[num+1]] = best_rolling_thresh . rolling_thresh = pd.DataFrame([rolling_thresh]).T.reset_index().rename(columns={&#39;index&#39;:&#39;month/year_index&#39;, 0:&#39;Fcst Thresh&#39;}) . rolling_thresh[&#39;Fcst Thresh&#39;].plot(figsize=(16,8), grid=True); . Above is very simliar to before but now we are trying out various moving windows along with various threshold percents. This is getting quite complex. No wonder Quants make so much money. As you can see from above the thresh percents change over time. Now let&#39;s see how we did. . df[&#39;yhat_optimized&#39;] = pd.merge(df, rolling_thresh, on=&#39;month/year_index&#39;, how=&#39;left&#39;)[&#39;Fcst Thresh&#39;].fillna(1).shift(1) * df[&#39;yhat_lower&#39;] . df[&#39;Prophet Rolling Thresh&#39;] = ((df[&#39;y&#39;] &gt; df[&#39;yhat_optimized&#39;]).shift(1)* (df[&#39;Percent Change&#39;]) + 1).cumprod() . (df.dropna().set_index(&#39;ds&#39;)[[&#39;Hold&#39;, &#39;Prophet&#39;, &#39;Prophet Thresh&#39;, &#39;Prophet Fcst Thresh&#39;, &#39;Prophet Rolling Thresh&#39;]] * 1000).plot(figsize=(16,8), grid=True) print(f&quot;Hold = {df[&#39;Hold&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet = {df[&#39;Prophet&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Thresh = {df[&#39;Prophet Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) # print(f&quot;Seasonality = {df[&#39;Seasonality&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Fcst Thresh = {df[&#39;Prophet Fcst Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Rolling Thresh = {df[&#39;Prophet Rolling Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) . Hold = 24,396 Prophet = 13,366 Prophet Thresh = 17,087 Prophet Fcst Thresh = 20,620 Prophet Rolling Thresh = 23,621 . As you can see our new Porphet Rolling Thresh did pretty well but still did&#39;t beat out the simpliest Hold strategy. Perhaps the saying &quot;Time in the Market is better then Timing the Market&quot; has some truth to it. . df[&#39;Time Traveler&#39;] = ((df[&#39;y&#39;].shift(-1) &gt; df[&#39;yhat&#39;]).shift(1) * (df[&#39;Percent Change&#39;]) + 1).cumprod() . (df.dropna().set_index(&#39;ds&#39;)[[&#39;Hold&#39;, &#39;Prophet&#39;, &#39;Prophet Thresh&#39;, &#39;Prophet Fcst Thresh&#39;, &#39;Prophet Rolling Thresh&#39;, &#39;Time Traveler&#39;]] * 1000).plot(figsize=(16,8), grid=True) print(f&quot;Hold = {df[&#39;Hold&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet = {df[&#39;Prophet&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Thresh = {df[&#39;Prophet Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) # print(f&quot;Seasonality = {df[&#39;Seasonality&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Fcst Thresh = {df[&#39;Prophet Fcst Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Prophet Rolling Thresh = {df[&#39;Prophet Rolling Thresh&#39;].iloc[-1]*1000:,.0f}&quot;) print(f&quot;Time Traveler = {df[&#39;Time Traveler&#39;].iloc[-1]*1000:,.0f}&quot;) . Hold = 24,396 Prophet = 13,366 Prophet Thresh = 17,087 Prophet Fcst Thresh = 20,620 Prophet Rolling Thresh = 23,621 Time Traveler = 288,513 . Above I implemented my Time Traveler strategy. This of course would be a perfect trading strategy as I know in advance when the market moves up or down. As you can the most you could make is $288,513 from $1,000. . Summary . Time Series Forecasting can be quite complex however Prophet makes it very easy to create robust forecasts with little effort. While it didn&#39;t make us rich with its stock market predictions it is still very useful and can be implemented quickly to solve many use cases in various areas. .",
            "url": "https://gardnmi.github.io/blog/jupyter,/prophet,/stock,/python/2020/10/20/stock-price-forecast-with-prophet.html",
            "relUrl": "/jupyter,/prophet,/stock,/python/2020/10/20/stock-price-forecast-with-prophet.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Predicting Reddit Comment Score with Keras Deep Learning",
            "content": "The code for this project can be found here. . . Imports . import os import requests import json import numpy as np import pandas as pd import zstandard as zstd import tensorflow as tf import seaborn as sns import tensorflow.keras.backend as K import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix from sklearn.preprocessing import StandardScaler from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Embedding, LSTM, Dropout from tensorflow.keras.models import Model, load_model from tensorflow.keras.initializers import Constant # Display Options pd.options.display.float_format = &#39;{:,.2f}&#39;.format #Variables MAX_SEQUENCE_LENGTH = 700 MAX_NUM_WORDS = 10000 EMBEDDING_DIM = 100 . The main libraries used for this project are Tensorflow and the high level API Keras which enable fast prototyping for projects and research. Tensorflow has integrated Keras into its library and can now be accessed directly from Tensorflow. We also import Pandas, Numpy, and Scikit-Learn to assist in our data wrangling . . Data Wrangling . There are several ways to grab comment data from Reddit but I found the fastest and most comprehensive way was to use pushift. which has already done a bulk of the work for us. Each file contains comments made on Reddit for a particular time period. Since each file contains millions of comments we are only going to download the most recent file. . # https://www.reddit.com/r/pushshift/comments/ajmcc0/information_and_code_examples_on_how_to_use_the/ fields = [&#39;body&#39;, &#39;created_utc&#39;, &#39;id&#39;, &#39;is_submitter&#39;, &#39;link_id&#39;, &#39;score&#39;, &#39;subreddit_id&#39;, &#39;subreddit&#39;] with open(&quot;RC_2019-02.zst&quot;, &#39;rb&#39;) as fh: dctx = zstd.ZstdDecompressor() with dctx.stream_reader(fh) as reader: previous_line = &quot;&quot; count = 1 while True: chunk = reader.read(2**24) if not chunk: break string_data = chunk.decode(&#39;utf-8&#39;) lines = string_data.split(&quot; n&quot;) comments = [] for i, line in enumerate(lines[:-1]): if i == 0: line = previous_line + line object = json.loads(line) if object[&#39;body&#39;] in [&#39;[deleted]&#39;, &#39;[removed]&#39;] or object[&#39;parent_id&#39;].startswith(&#39;t1&#39;) or object[&#39;subreddit&#39;].lower() not in [&#39;politics&#39;]: pass else: object = dict((k, object[k]) for k in fields if k in object) comments.append(object) df = pd.DataFrame(comments) df.to_csv(f&#39;raw-data//{count}_comments.csv&#39;, index=False) count += 1 previous_line = lines[-1] . The zst file is too big to read into memory all at once. The above code loops through the file from pushift and stores chunks of comments into individual csv files. In order to further reduce the size we also limit the comments to the politics subreddit. The politics subreddit has a consistent narrative therefore it might not be as eratic as other subreddits when it comes to comment score. . url = &#39;https://api.pushshift.io/reddit/search/submission/?ids=&#39; for file in os.listdir(r&#39;raw-data/&#39;): link_id = [] link_created_utc = [] df = pd.read_csv(f&#39;raw-data/{file}&#39;) link_ids = df[&#39;link_id&#39;].str.split(&#39;_&#39;, expand=True)[1].unique().tolist() split_link_ids = np.array_split(link_ids, 5) try: for ids in split_link_ids: r = requests.get(f&#39;{url}{&quot;,&quot;.join(ids)}&#39;) package_json = r.json() data = package_json[&#39;data&#39;] for field in data: link_id.append(&#39;t3_&#39; + field[&#39;id&#39;]) link_created_utc.append(field[&#39;created_utc&#39;]) data = list(zip(link_id, link_created_utc)) link_df = pd.DataFrame(data, columns=[&#39;link_id&#39;, &#39;link_created_utc&#39;]) new_df = df.merge(link_df, on=&#39;link_id&#39;, how=&#39;inner&#39;) new_df.to_csv(f&#39;comb-data/{file}&#39;, index=False) except: print(&#39;failed&#39;) pass . The comment data is missing the submission timestamp for which the comment belongs too. Time elapsed since submission is a critical factor in determining comment score so we need this information. Fortunately, the same site that provided the comment data also has an API we can leverage to fill in our missing data. . The above script opens up each csv file that was created earlier and passes the post id (link_id) to the API in which returns the post timestamp. We then save the csv file with our new field.. . filepaths = [f&#39;comb-data/{file}&#39; for file in os.listdir(r&#39;comb-data/&#39;)] df = pd.concat(map(pd.read_csv, filepaths)) df = df.reset_index(drop=True) df.to_csv(&#39;reddit_data.csv&#39;, index=False) . Finally we take the individual csv files and merge them into a single csv file. We should try and reduce as much complexity as we can. You could feed the individual csv files to Keras model but if you don&#39;t have to then don&#39;t do it. . . Data Exploration . # drive.mount(&#39;/content/gdrive&#39;) # df = pd.read_csv(r&#39;gdrive/My Drive/Projects/reddit-score-pred/reddit_data.csv&#39;) # embeddings_index = {} # with open(&#39;gdrive/My Drive/Projects/reddit-score-pred/glove.6B.100d.txt&#39;, encoding=&quot;utf8&quot;) as f: # for line in f: # word, coefs = line.split(maxsplit=1) # coefs = np.fromstring(coefs, &#39;f&#39;, sep=&#39; &#39;) # embeddings_index[word] = coefs . df = pd.read_csv(&#39;reddit_data.csv&#39;) df = df[df.subreddit == &#39;politics&#39;] . df.head() . body created_utc id is_submitter link_id score subreddit subreddit_id link_created_utc . 3 I&#39;ll be honest I&#39;m not a fan of this. n nI re... | 1549297521 | efqd1ln | False | t3_an1zxq | -10 | politics | t5_2cneq | 1549289373 | . 4 Trump curse incoming!!! | 1549297533 | efqd26p | False | t3_an1zxq | 1 | politics | t5_2cneq | 1549289373 | . 5 How is he going to get cold Chinese food at th... | 1549297561 | efqd3l0 | False | t3_an1zxq | 2 | politics | t5_2cneq | 1549289373 | . 6 The one backbencher for the Pats likes Obama. ... | 1549297594 | efqd57i | False | t3_an1zxq | -12 | politics | t5_2cneq | 1549289373 | . 7 What&#39;s Obama up to these days? | 1549297606 | efqd5ri | False | t3_an1zxq | 1 | politics | t5_2cneq | 1549289373 | . df.body = df.body.astype(str) print(&#39;Comment Metrics&#39;) print(f&#39;Mean: {df.body.map(len).mean():.0f}&#39;) print(f&#39;Min: {df.body.map(len).min():.0f}&#39;) print(f&#39;Max: {df.body.map(len).max():.0f}&#39;) print(f&#39;Std: {df.body.map(len).std():.0f}&#39;) . Comment Metrics Mean: 195 Min: 1 Max: 10123 Std: 304 . df.score.describe() . count 506,386.00 mean 19.50 std 213.89 min -491.00 25% 1.00 50% 2.00 75% 7.00 max 21,364.00 Name: score, dtype: float64 . Our dataset contains over 500,000 comments. When looking at the distribution of the scores it appears we may have some issues as it appears we don&#39;t have a normal distribution. Below we&#39;ll plot some charts of the distribution and various normalization techniques. . plt.figure(figsize=(15, 5)) sns.distplot(df.score); . scaler = StandardScaler() scores = scaler.fit_transform(df.score.values.reshape(-1,1)) plt.figure(figsize=(15, 5)) sns.distplot(scores); . C: Users Mike .conda envs reddit-score-pred lib site-packages sklearn utils validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler. warnings.warn(msg, DataConversionWarning) C: Users Mike .conda envs reddit-score-pred lib site-packages sklearn utils validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler. warnings.warn(msg, DataConversionWarning) . from sklearn.preprocessing import power_transform scores = power_transform(df.score.values.reshape(-1,1), method=&#39;yeo-johnson&#39;) plt.figure(figsize=(15, 5)) sns.distplot(scores); . from sklearn.preprocessing import quantile_transform scores = quantile_transform(df.score.values.reshape(-1,1), n_quantiles=99, random_state=0) plt.figure(figsize=(15, 5)) sns.distplot(scores); . from sklearn.preprocessing import robust_scale scores = robust_scale(df.score.values.reshape(-1,1)) plt.figure(figsize=(15, 5)) sns.distplot(scores); . df.score.quantile(np.linspace(.04, 1, 24, 0)) . 0.04 -6.00 0.08 -1.00 0.12 0.00 0.16 1.00 0.20 1.00 0.24 1.00 0.28 1.00 0.32 1.00 0.36 1.00 0.40 1.00 0.44 2.00 0.48 2.00 0.52 2.00 0.56 3.00 0.60 3.00 0.64 4.00 0.68 5.00 0.72 6.00 0.76 7.00 0.80 9.00 0.84 12.00 0.88 16.00 0.92 24.00 0.96 48.00 Name: score, dtype: float64 . After exploring the distribution it appears that we may have some issues if we try to predict the score using regression. A better option would be to bin the data into categorical groups and use a classification model to predict groups of data. . pd.cut( df.score, [-10000, 0, 1, 10, 50, 10000], labels=[&#39;Downvoted&#39;, &#39;No Votes&#39;, &#39;2-10 Votes&#39;, &#39;11-50 Votes&#39;, &#39;50+ Votes&#39;]).value_counts() . 2-10 Votes 203917 No Votes 151028 11-50 Votes 69650 Downvoted 62430 50+ Votes 19321 Name: score, dtype: int64 . df[&#39;score_category&#39;] = pd.cut( df.score, [-100000, 0, 1, 10, 50, 100000], labels=[&#39;Downvoted&#39;, &#39;No Votes&#39;, &#39;2-10 Votes&#39;, &#39;11-50 Votes&#39;, &#39;50+ Votes&#39;]) . df.score_category.cat.categories . Index([&#39;Downvoted&#39;, &#39;No Votes&#39;, &#39;2-10 Votes&#39;, &#39;11-50 Votes&#39;, &#39;50+ Votes&#39;], dtype=&#39;object&#39;) . pd.concat([df.score_category, df.score_category.cat.codes], axis=1).head(10) . score_category 0 . 3 Downvoted | 0 | . 4 No Votes | 1 | . 5 2-10 Votes | 2 | . 6 Downvoted | 0 | . 7 No Votes | 1 | . 8 2-10 Votes | 2 | . 9 11-50 Votes | 3 | . 10 No Votes | 1 | . 11 Downvoted | 0 | . 12 2-10 Votes | 2 | . We utilize the pandas cut method to group our data into 5 groups (&#39;Downvoted&#39;, &#39;No Votes&#39;, &#39;2-10 Votes&#39;, &#39;11-50 Votes&#39;, &#39;50+ Votes&#39;). You can access a numerical representation of the category by callin the .cat.codes method. . . Data Preperation . df[&#39;time_lapsed&#39;] = df[&#39;created_utc&#39;] - df[&#39;link_created_utc&#39;] . The only feature engineering I did for this project is to create a field to record the time lapsed between the submission timestamp and comment timestamp. However, it isn&#39;t a bad idea to spend time here and feature engineer more fields such as minute, hour, day of week, month, etc. . embeddings_index = {} with open(&#39;glove.6B.100d.txt&#39;, encoding=&quot;utf8&quot;) as f: for line in f: word, coefs = line.split(maxsplit=1) coefs = np.fromstring(coefs, &#39;f&#39;, sep=&#39; &#39;) embeddings_index[word] = coefs tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) tokenizer.fit_on_texts(df.body) sequences = tokenizer.texts_to_sequences(df.body) word_index = tokenizer.word_index comments = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) # Prepare Embedding Matrix num_words = min(MAX_NUM_WORDS, len(word_index)) + 1 embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i &gt; MAX_NUM_WORDS: continue embedding_vector = embeddings_index.get(word) if embedding_vector is not None: # words not found in embedding index will be all-zeros. embedding_matrix[i] = embedding_vector print(f&#39;Found {len(word_index):,} unique tokens.&#39;) . Found 142,859 unique tokens. . To prep our comments for our Deep Learning model we will be tokenizing (indexing) each unique word found in all our comments. We then use a technique called Embedding to assign a vector of numbers to each unique token. The embedding values we are using are pretrained values and you can read more about them in this keras blog post. . pd.Series(word_index).head(10) . the 1 to 2 a 3 and 4 of 5 this 6 is 7 in 8 i 9 for 10 dtype: int64 . comments . array([[ 0, 0, 0, ..., 477, 1488, 130], [ 0, 0, 0, ..., 0, 27, 4701], [ 0, 0, 0, ..., 21, 508, 168], ..., [ 0, 0, 0, ..., 757, 79, 1019], [ 0, 0, 0, ..., 3, 2087, 1116], [ 0, 0, 0, ..., 8, 1171, 11]]) . embedding_matrix[word_index[&#39;trump&#39;]] . array([-0.15730999, -0.75502998, 0.36844999, -0.18957999, -0.16896001, -0.23157001, -0.22657999, -0.30186 , 0.24372 , 0.61896002, 0.58995003, 0.047638 , -0.055164 , -0.70210999, 0.22084001, -0.69231999, 0.49419001, 1.42850006, -0.25362 , 0.20031001, -0.26192001, 0.05315 , -0.048418 , -0.44982001, 0.54644001, -0.014645 , -0.015531 , -0.61197001, -0.91964 , -0.75279999, 0.64842999, 1.0934 , 0.052682 , 0.33344999, 0.10532 , 0.59517002, 0.023104 , -0.37105 , 0.29749 , -0.23683 , 0.079566 , -0.10326 , 0.35885 , -0.28935 , -0.19881 , 0.22908001, -0.061435 , 0.56127 , -0.017115 , -0.32868001, -0.78416997, -0.49375001, 0.34944001, 0.16278 , -0.061168 , -1.31060004, 0.39151999, 0.124 , -0.20873 , -0.18472999, -0.56184 , 0.55693001, 0.012114 , -0.54544997, -0.31409001, 0.1 , 0.31542999, 0.74756998, -0.47734001, -0.18332 , -0.65622997, 0.40768 , -0.30697 , -0.47246999, -0.7421 , -0.44977999, -0.078122 , -0.52673 , -0.70633 , 1.32710004, 0.26298001, -0.91000003, 0.91632003, -0.51643002, 0.20284 , -0.25402001, -1.25660002, 0.20271 , 0.92105001, -0.57573998, -0.15105 , -0.24831 , 0.36673 , -0.53987002, 0.18534 , 0.25713 , 0.38793999, -0.54136997, 0.67817003, -0.17251 ]) . Above we can see each word token, the tokens in an array representing our comment, and the embedding for the word &quot;trump&quot;. . features = df.drop([&#39;score&#39;, &#39;score_category&#39;, &#39;body&#39;, &#39;id&#39;, &#39;link_id&#39;, &#39;subreddit&#39;, &#39;subreddit_id&#39;], axis=1) category = df[&#39;score_category&#39;].cat.codes # Scaling Features for Model scaler = StandardScaler() features = scaler.fit_transform(features) . C: Users Mike .conda envs reddit-score-pred lib site-packages sklearn preprocessing data.py:645: DataConversionWarning: Data with input dtype bool, int64 were all converted to float64 by StandardScaler. return self.partial_fit(X, y) C: Users Mike .conda envs reddit-score-pred lib site-packages sklearn base.py:464: DataConversionWarning: Data with input dtype bool, int64 were all converted to float64 by StandardScaler. return self.fit(X, **fit_params).transform(X) . We create a features column by dropping the fields we won&#39;t be using. We also create a separate array for the &quot;score categories&quot; which is what we are trying to predict. Finally, we scale our features using the scikit-learn &quot;StandardScaler&quot; method. Scaling our data makes it a lot easier for our model to optimize our loss function. There is also a third array &quot;comments&quot; not shown above that is an additional feature array that we will be feeding to the model. . comments_train, comments_val, feature_train, feature_val, category_train, category_val = train_test_split( comments, features, category, test_size=0.33, random_state=42) . Finally we split our data into a Training set and Validation set using scikit-learn &quot;train_test_split&quot; function. We will be creating a multi-input and single output Keras model so we have two sets of feature data (comments and features). . . Model Creation . # note that we set trainable = False so as to keep the embeddings fixed embedding_layer = Embedding( num_words, EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_SEQUENCE_LENGTH, trainable=False) . The first thing we do is setup our embedding layer. You can find out more about the layer above in this Keras Blog. . comments_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype=&#39;int32&#39;) embedded_sequences = embedding_layer(comments_input) x = Conv1D(128, 5, activation=&#39;relu&#39;)(embedded_sequences) x = MaxPooling1D(5)(x) x = Conv1D(128, 5, activation=&#39;relu&#39;)(x) x = MaxPooling1D(5)(x) x = Conv1D(128, 5, activation=&#39;relu&#39;)(x) x = GlobalMaxPooling1D()(x) x = Dropout(.3)(x) # comments_prediction = Dense(5, activation=&#39;softmax&#39;)(x) comments_output = Dense(128, activation=&#39;relu&#39;)(x) feature_input = Input(shape=(4, )) x = tf.keras.layers.concatenate([comments_output, feature_input]) # We stack a deep densely-connected network on top x = Dense(250, activation=&#39;relu&#39;)(x) x = Dropout(.3)(x) x = Dense(125, activation=&#39;relu&#39;)(x) x = Dropout(.3)(x) # And finally we add the main logistic regression layer final_prediction = Dense(5, activation=&#39;softmax&#39;)(x) . For the core of our Keras Model we will be utilizing the Functional API. It&#39;s based on the Multi Input and Multi Output example found on the Keras documentation. The first input feature will be our comments and our second input feature will be the other feature columns . model = Model([comments_input, feature_input], [final_prediction]) model.compile( loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;accuracy&#39;]) model.fit([comments_train, feature_train], [category_train], batch_size=128, epochs=100, validation_data=([comments_val, feature_val], [category_val])) . The last step is to Initialize our model with the layers we created and define the loss function and gradient descent optimizer. We can also pass other metrics such as &quot;accuracy&quot; that Keras will track for us. Finally, we fit the model using the training and validation we created earlier. Above I have set the epochs to 100 which tells our model to loop through our training dataset 100 times, each time optimizing the weights and bias to reduce our loss. . pd.DataFrame(model.history.history)[[&#39;loss&#39;,&#39;val_loss&#39;]].plot(figsize=(15, 5)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff3e41a0358&gt; . We then graph the results of our loss. Above you can see that our train loss flattens around 35 epochs and then jumps around. Our validation loss is best at only 2 or 3 epochs. The validation loss gets worse the more we overfit our model to our training data. . model.save(&#39;reddit_score.h5&#39;) # creates a HDF5 file &#39;my_model.h5&#39; del model # deletes the existing model # returns a compiled model model = load_model(&#39;reddit_score.h5&#39;) . If we want to at this point we can choose to save our model using the &quot;save&quot; method. We can also load the saved model just as easily. . model = Model([comments_input, feature_input], [final_prediction]) model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;accuracy&#39;]) model.fit([comments_train, feature_train], [category_train], batch_size=128, epochs=3, validation_data=([comments_val, feature_val], [category_val])) . Train on 339278 samples, validate on 167108 samples Epoch 1/3 339278/339278 [==============================] - 1172s 3ms/sample - loss: 1.2279 - accuracy: 0.4943 - val_loss: 1.1993 - val_accuracy: 0.4988 Epoch 2/3 339278/339278 [==============================] - 1143s 3ms/sample - loss: 1.2009 - accuracy: 0.5007 - val_loss: 1.1884 - val_accuracy: 0.5014 Epoch 3/3 339278/339278 [==============================] - 1173s 3ms/sample - loss: 1.1957 - accuracy: 0.5020 - val_loss: 1.1991 - val_accuracy: 0.4993 . &lt;tensorflow.python.keras.callbacks.History at 0x2249e8c5d68&gt; . model.history.history . {&#39;loss&#39;: [1.2278857953891122, 1.2008990119142613, 1.1957352068886913], &#39;accuracy&#39;: [0.4942643, 0.5007133, 0.50197774], &#39;val_loss&#39;: [1.1993256573285909, 1.1884006263822988, 1.1991039581740772], &#39;val_accuracy&#39;: [0.49880317, 0.50144815, 0.4993238]} . pd.DataFrame(model.history.history)[[&#39;loss&#39;,&#39;val_loss&#39;]].plot(figsize=(15, 5)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x22490643cc0&gt; . Since our model appears to do best at around 3 epochs I chose to reset the model and re-fit it with just 3 epochs. . . Model Evaluation . predictions = model.predict([comments_val, feature_val]) prediction_ = np.argmax(predictions, axis = 1) . To further evaluate the model we need to make a set of predictions. Since I didn&#39;t create a holdout set of data I went ahead and made predictions on our validation data. The predict method returns an array of probabilities of each &quot;score category&quot;. In order to choose the score with the highest probability, we leverage numpy&#39;s argmax method to grab the index of the highest probability. . confusion_matrix(category_val, prediction_) . array([[ 1082, 7290, 12311, 0, 0], [ 412, 28688, 20640, 4, 1], [ 621, 12996, 53664, 3, 1], [ 246, 1031, 21744, 5, 4], [ 51, 115, 6191, 6, 2]], dtype=int64) . To visualize our accuracy we use the scikit-learn confusion matrix. However, it&#39;s not very descriptive by itself. . def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14): &quot;&quot;&quot; Arguments confusion_matrix: numpy.ndarray The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. Similarly constructed ndarrays can also be used. class_names: list An ordered list of class names, in the order they index the given confusion matrix. figsize: tuple A 2-long tuple, the first value determining the horizontal size of the ouputted figure, the second determining the vertical size. Defaults to (10,7). fontsize: int Font size for axes labels. Defaults to 14. Returns - matplotlib.figure.Figure The resulting confusion matrix figure &quot;&quot;&quot; df_cm = pd.DataFrame( confusion_matrix, index=class_names, columns=class_names, ) fig = plt.figure(figsize=figsize) try: heatmap = sns.heatmap(df_cm, annot=True, fmt=&quot;d&quot;) except ValueError: raise ValueError(&quot;Confusion matrix values must be integers.&quot;) heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha=&#39;right&#39;, fontsize=fontsize) heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha=&#39;right&#39;, fontsize=fontsize) plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) return fig . print_confusion_matrix(confusion_matrix(category_val, prediction_), df.score_category.values.unique().tolist()); . The above script enhances our confusion matrix by leveraging seaborn. It provides a much clearer picture of are results. The code can be found here. . results = pd.concat([category_val, df.body], axis=1, join=&#39;inner&#39;).reset_index(drop=True) results = pd.concat([results, pd.Series(prediction_)], axis=1) results.columns = [&#39;Actual&#39;, &#39;Comment&#39;, &#39;Prediction&#39;] . results.head() . Actual Comment Prediction . 0 2 | Does Trump think El Paso is a Republican frien... | 1 | . 1 0 | The thumbnail is a great representation of Tuc... | 1 | . 2 2 | So the Emperor went in procession under the ri... | 1 | . 3 2 | I made a meme: https://youtu.be/D899XSiFt1s | 1 | . 4 4 | &amp;gt; No president ever worked harder than me (... | 0 | . results[(results.Actual == 4) &amp; (results.Prediction == 0) ].Comment.values . array([&#39;&amp;gt; No president ever worked harder than me (cleaning up the mess I inherited)! n n1. What [mess](https://i.imgur.com/aiIOGEh.png)? n n2. You created the &#34;crisis&#34; at the border, concerning a group of people that commit less violent crime than the general American public during a time when illegal immigration has been at historic lows for years now. n n3. The SCOTUS appointments fell into your lap. n n4. The ACA now has popular support from the American people. n n5. Most Americans do not want your stupid wall. n n6. You have given another huge tax break to wealth individuals and corporations, forcing the rest of us to continue to pay for their avarice. n n7. You have alienated our historic partnerships while coddling atrocious dictators. You have bent a knee to a nation (Russia) that has a smaller economy than Texas, California or New York... by ***themselves***. n n8. You have spent more time on the golf course and in front of a TV than any modern President. n n9. Your party was shellacked in the midterms, due to ***you***. n n10. You have the thinnest skin of any human being I have ever known. You hate facts if they are facts you do not like. &#39;, &#39;Lmfao. n nClassic GOP tactics n nFuck everything up and lose the elections and then relentlessly blame everything on the people trying to fix your fucking irresponsible disasters n n nIf the people in these states keep falling for this shit i cant have any sympathy for their woes any longer. n n nIm just fed the fuck up. n n nVote these pigs out or shut the fuck up because youre hurting yourselves by voting for the GOP&#39;, &#39;[There is a good argument to be made that reducing wealth inequality will make us all richer, even the rich.] (https://www.washingtonpost.com/news/wonk/wp/2018/02/06/how-rising-inequality-hurts-everyone-even-the-rich/?utm_term=.22e77b44f275) They should stop worrying about this so much, it will be good for everyone. &#39;, &#39;Uh...a lot of Republicans (largely your Evangelicals) don’t see bigotry against LGBTQ folks as bigotry. They think it’s the *right* thing to do. It’s like asking them to confront their right leg. &#39;, &#39;Remember last year when everyone came out to mute AOC when she said the word &#34;occupation&#34; regarding Israel. n nWe &#39;re seeing the full effect of the AIPAC.&#39;, &#34;James buchanon was 2nd worst and andrew johnson are the worst according to the survey. n nFor those who wanted to know. n n nThe only thing keep Trump out of that spot are two categories: luck, and willingness to take risks lol n nEdit: based on the replies, I am sure glad I&#39;m not the only one who had no idea who Andrew Johnson was, and confused him with Jackson. But no, they are different people.&#34;, &#39;&amp;gt;In the article published Tuesday morning, Wohl disclosed what he claimed were his plans to create “enormous left-wing properties” including Facebook and Twitter accounts before the 2020 presidential election in order “to steer the left-wing votes in the primaries to what we feel are weaker candidates compared with Trump.” n nEvery day that this chode is not behind bars is an affront to justice.&#39;, &#39;&amp;gt;&#34;I know most of the Democratic primary candidates are all talking about Medicare for all. I think instead we should do Medicare at 55,&#34; Brown said during a question and answer session at the Chamber of Commerce in Clear Lake, Iowa. Brown said that reducing the age or letting people over 55 buy into the existing Medicare system early would have a better chance of getting through Congress. n nNOPE. n nThis &#34;Medicare at 55&#34; business is a bit too 90s for my tastes. n nThat leaves a gap from 26 (when people are kicked off their parents insurance) until 55 which is straight up dumb. It &#39;s just a needless 30 year gap, and in that time the choices people will make in regards to their health will greatly impact how much medical care they &#39;re going to need as they get elderly. A whole lot of family planning happening in that gap too. n nPreventative medicine is one of many reasons Medicare for all makes sense. 30 year olds shouldn &#39;t have to stress about doctor bills. Not a 36 year old, a 41 year old, or a 53 year old. Nobody should have to stress about doctor bills. People who are sick have enough to worry about. n nAnd for fuck &#39;s sake no one in America should go *bankrupt* from medical bills. Ever. n nSen. Brown should rethink his position. Or get the hell out of the way.&#39;, &#34;Tell &#39;em AOC! They try to fear monger with the S-word boogeyman, but progressive policies are almost all very popular: n n n70% Medicare For All n n[https://thehill.com/policy/healthcare/403248-poll-seventy-percent-of-americans-support-medicare-for-all](https://thehill.com/policy/healthcare/403248-poll-seventy-percent-of-americans-support-medicare-for-all) n n&amp;amp;#x200B; n n63% $15 Minimum Wage n n[https://www.nelp.org/wp-content/uploads/Minimum-Wage-Basics-Polling.pdf](https://www.nelp.org/wp-content/uploads/Minimum-Wage-Basics-Polling.pdf) n n&amp;amp;#x200B; n n60% Tuition Free College n n[https://www.reuters.com/investigates/special-report/usa-election-progressives/](https://www.reuters.com/investigates/special-report/usa-election-progressives/) n n&amp;amp;#x200B; n n81% Green New Deal n n[https://www.huffingtonpost.com/entry/green-new-deal-poll _us _5c169f2ae4b05d7e5d8332a5](https://www.huffingtonpost.com/entry/green-new-deal-poll_us_5c169f2ae4b05d7e5d8332a5) n n t n n59% A 70% Top Marginal Tax Rate n n[https://thehill.com/hilltv/what-americas-thinking/425422-a-majority-of-americans-support-raising-the-top-tax-rate-to-70](https://thehill.com/hilltv/what-americas-thinking/425422-a-majority-of-americans-support-raising-the-top-tax-rate-to-70) n n&amp;amp;#x200B; n n72% Expanding Social Security n n[https://socialsecurityworks.org/2016/10/26/new-polling-americans-are-united-in-support-of-expanding-social-security/](https://socialsecurityworks.org/2016/10/26/new-polling-americans-are-united-in-support-of-expanding-social-security/) n n&amp;amp;#x200B; n n62% Legalizing Cannabis n n[http://www.pewresearch.org/fact-tank/2018/10/08/americans-support-marijuana-legalization/](http://www.pewresearch.org/fact-tank/2018/10/08/americans-support-marijuana-legalization/) n n&amp;amp;#x200B; n n65% Reform Racist Incarceration System n n[https://www.politico.com/f/?id=00000161-2ccc-da2c-a963-efff82be0001](https://www.politico.com/f/?id=00000161-2ccc-da2c-a963-efff82be0001) n n&amp;amp;#x200B; n n63% Same Sex Marriage Freedom n n[https://news.gallup.com/poll/234866/two-three-americans-support-sex-marriage.aspx](https://news.gallup.com/poll/234866/two-three-americans-support-sex-marriage.aspx) n n&amp;amp;#x200B; n n69% Keep Roe vs. Wade n n[https://thinkprogress.org/pro-choice-america-majority-d8963029ae45/](https://thinkprogress.org/pro-choice-america-majority-d8963029ae45/) n n&amp;amp;#x200B; n n81% Undocumented Migrant Path to Citizenship [https://www.newsweek.com/more-80-americans-want-undocumented-immigrants-have-chance-become-us-citizens-1316889](https://www.newsweek.com/more-80-americans-want-undocumented-immigrants-have-chance-become-us-citizens-1316889)&#34;, &#39;So it was probably the WH trying to spin that he was done? n nEdit I should say someone the WH pushed at DOJ to speed things up. I’m fairly confident special counsel is almost done, but all this guessing game stuff just reeks of agendas being pushed. &#39;, &#34;Sorry. It&#39;s ours now. Be glad we still support the red taker states and kindly STFU. &#34;, &#39;I feel like Rep. Tim Ryan picked a really terrible time to do an AMA. n nedit: We should hit him up on twitter afterwards. Not the smartest move in terms of scheduling, but I at least appreciate the outreach efforts. @RepTimRyan&#39;, &#39;Tulsi Gabbard comes from a family of conservative activists, most famous for their opposition to gay marriage in Hawaii: https://www.jacobinmag.com/2017/05/tulsi-gabbard-president-sanders-democratic-party n nTulsi Gabbard has said her personal views on LGBT equality haven &#39;t changed as recently as 2015: https://www.ozy.com/rising-stars/tulsi-gabbard-a-young-star-headed-for-the-cabinet/62604 n nTulsi Gabbard is rated &#34;F&#34; by Progressive Punch for voting with Republicans, despite the strong progressive lean of her district: https://imgur.com/wDhVNKq n nTulsi Gabbard was nearly a part of Trump &#39;s cabinet at Steve bannon &#39;s suggestion: https://abcnews.go.com/Politics/democratic-rep-tulsi-gabbard-consideration-trump-cabinet/story?id=43696303 n nhttps://thehill.com/homenews/administration/307106-bannon-set-up-trump-gabbard-meeting n nTulsi Gabbard has also been praised multiple times by Steve Bannon, Trump &#39;s former strategist and prolific white nationalist propagandist: http://www.hawaiinewsnow.com/story/36352314/bannon-name-drops-hawaii-congresswoman-in-national-interview/ n nTulsi Gabbard declined to join 169 Democrats in condemning Trump for appointing Steve Bannon to his cabinet: https://mauitime.com/news/politics/why-didnt-rep-tulsi-gabbard-join-169-of-her-colleagues-in-denouncing-trump-appointee-stephen-bannon/ n nTulsi Gabbard isn &#39;t anti-war. She &#39;s a self-described hawk against terrorists. Her narrow objections center around efforts to spread democracy: &#34;In short, when it comes to the war against terrorists, I &#39;m a hawk,&#34; Gabbard said. &#34;When it comes to counterproductive wars of regime change, I &#39;m a dove.&#34;: https://www.votetulsi.com/node/27796 n nTulsi Gabbard copies the rhetoric of Republicans: Gabbard voted against condemning Bashar al-Assad, president of Syria, and was praised by conservative media for publicly challenging President Barack Obama over his refusal to use the term &#34;Islamic extremism&#34; when discussing terrorism: https://www.washingtontimes.com/news/2015/jan/28/tulsi-gabbard-slams-obamas-refusal-to-say-islamic-/ n nTulsi Gabbard also copies the policy of Republicans, voting with them to block Syrian refugees: https://medium.com/@pplswar/tulsi-gabbard-voted-to-make-it-virtually-impossible-for-syrian-refugees-to-come-to-the-u-s-11463d0a7a5a n nTulsi Gabbard has multiple connections to Hindu nationalists: https://www.alternet.org/civil-liberties/curious-islamophobic-politics-dem-congressmember-tulsi-gabbard n nTulsi Gabbard frequently repeats Russian talking points and works to legitimize Assad: https://www.theguardian.com/us-news/2017/jan/26/tulsi-gabbard-bashar-al-assad-syria-democrats n nTulsi Gabbard used her time during the congressional investigation into Russian interference to engage in blatant whataboutism to attack the US and to deflect for Russia: https://www.youtube.com/watch?v=mkigREtfAT0 n nTulsi Gabbard was one of only 3 representatives to not condemn Assad for gassing Syrian civilians and the only Democrat: https://www.congress.gov/bill/114th-congress/house-concurrent-resolution/121/text n nTulsi Gabbard has introduced legislation pushed by GOP-megadonor, Sheldon Adelson: https://www.reuters.com/article/us-usa-politics-adelson-idUSBREA2P0BJ20140326 n nTulsi was later awarded a &#34;Champions of Freedom&#34; medal at Adelson &#39;s annual gala in 2016: https://www.thedailybeast.com/tulsi-gabbard-the-bernie-endorsing-congresswoman-who-trump-fans-can-love&#39;, &#34;And people wonders why I have a hard time coming out to people. People like this asshole fuels my anxiety n nEdit: wow guys. I got off work and didn&#39;t expect such a positive response. I wrote this post when I was feeling depressed about life and it changes my day around. &#34;, &#39;Actions he tries to take during a State of Emergency are still subject to legal challenge in court. He &#39;ll basically have to prove the &#34;emergency&#34; if he doesn &#39;t want to be stopped by **all the injunctions.** Landowners on the border still have rights, Eminent Domain still takes a certain amount of time and legal challenge. n nEdit: [DOJ has warned Trump that courts are likely to block this.](https://thehill.com/homenews/administration/430122-doj-warns-white-house-that-national-emergency-will-likely-be-blocked?__twitter_impression=true)&#39;, &#39;This is a good thing. We need someone to challenge Trump. After the shutdown stunt, he certainly have lost quite a few voters. &#39;, &#39;This seat was blue up until 2015, Gillibrand and Cuomo both won it. Hopefully the DCCC takes a hard look at NY-01 in the next cycle. &#39;, &#39;This is why I just laugh when people say a corporate tax is a double taxation. First of all, all taxation is &#34;double taxation&#34;, because taxes accrue on a bunch of stuff. Is a sales tax double taxation? Are property taxes? Heck we tax Social Security as income, and that comes straight from taxes. All taxes just occur at different stages in the economy. But second, and most scary, is that thanks to various loopholes, most capital gains at the individual level aren &#39;t taxed at all (IIRC, [something like 2/3rds isn &#39;t taxed](https://www.ctj.org/fact-sheet-why-we-need-the-corporate-income-tax/)), so if corporations weren &#39;t taxed, and individuals weren &#39;t taxed, then no taxes would be placed on a lot of wealth generated for shareholders. n nEdit: added source.&#39;, &#39;&amp;gt;One week from today, President Trump must tell Congress whether he believes Saudi Crown Prince Mohammed Bin Salman is responsible for killing Washington Post columnist Jamal Khashoggi. However, this assumes that Trump respects a requirement from Congress. n nOdds of Trump meeting the deadline? n nEDIT: I guarantee that, in Trump &#39;s head, the status of Khashoggi &#39;s murder by MBS is &#34;blown over&#34;. Same with Kushner.&#39;, &#34;What&#39;s funny is that while O&#39;Rourke of course is on the correct side of most/all of the major issues, he was hardly known as a flaming liberal (before going viral) being a representative from Texas. In fact: n n&amp;gt;GovTrack placed O&#39;Rourke near the ideological center of the Democratic Party; the American Civil Liberties Union gave him an 88 percent rating, while the United States Chamber of Commerce, a more conservative group, gave him a 47 percent rating. [131 ] According to FiveThirtyEight, which tracks Congressional voting records, O&#39;Rourke voted in line with Donald Trump 30.1 percent of the time during the 115th Congress. [132 ] n nIt really goes to show you how utterly vile and effective the Fox News demonization machine is. In a less fucked up world, this guy could have been grudgingly voting for Beto rather than trying to murder him.&#34;, &#39;Everything is “extreme” when a democrat does it. When its a republican caging babies and banning muslims, its just “policy”. &#39;, &#39;Translation: Dems know they’ve won this issue, Trump too stupid to give in.&#39;, &#39;Its funny to see white nationalists so up in arms about anti-Semitism when its convenient. n nEdit:I know what she said is ***not*** anti-Semitic. It was more of a comment on how that its being used against her that way by a racist minority of individuals and other beholden to political donations.&#39;, &#39;&amp;gt; When Christie was fired on Nov. 11, Flynn and former White House chief strategist Steve Bannon tossed binders full of potential personnel picks into the trash to celebrate the departure. n nhttps://www.politico.com/states/new-jersey/story/2017/12/06/christie-warning-about-flynn-among-reasons-i-was-fired-from-trump-transition-136432&#39;, &#39;would really help if the American media stoped butchering the word &#34;socialism&#34;. What you guys call socialist is so far from the definition &#39;, &#39;The time of the centrist neoliberal Democrat is over. For too long has the left been told to &#34;compromise&#34; in service of allegedly getting things accomplished, when all that &#39;s been done the last 40 years is advancing the agenda of the Billionaire/banker class, increased US imperialism and interventionism, and destruction of the earth &#39;s climate. n nBernie Sanders &#39; time has come. His movement has come. Even the corporate media and all the super delegates in the world can &#39;t stop him this time.&#39;, &#39;Back before it seemed like Trump had any chance of becoming president, my idiot cousin was wearing a MAGA hat. I thought Trump’s candidacy was such a farce, that I had my husband take a pic of me wearing that hat, huge grin on my face, and sent it to a couple friends as a joke... Not so funny now... &#39;, &#34;Why can&#39;t we pay student loans with pre-tax money?! n nMost decent paying jobs require a college degree. To go to college, most people HAVE to take out a student loan, usually through the government. Then, if you actually get a job, you have to pay taxes on the money you&#39;re just gonna give right back to the govt with interest. They&#39;re boning us from every direction. n nThen if you actually make a decent amount of money because you went to college to get s good job, you don&#39;t qualify anymore to deduct student loan interest, even though you might be paying 10k a year of your salary just in student loans. n nIf the money we paid back towards student loans was deducted from our tax liability it would help so much. &#34;, &#39;Steve Schmidt was complaining about liberals bullying Howard Schultz on MSNBC yesterday and all I could think was... &#34;Really? Your first move to position him as a potential candidate for POTUS is to play him as a victim?&#34;&#39;, &#34;My fear is that if they&#39;re really trying to bury this, that when we do eventually find out, it will just be viewed thru a political lens. Not a legal one. Much like Benghazi where the Democrats know it was BS and the Republicans think it&#39;s worse than colluding with Russians to steal an election. The report doesn&#39;t mean anything. Indictments are the only thing that the public will believe. And even then… some will never believe.&#34;, &#34;It&#39;s the transparency that scares them... n n[McConnell, Rubio, McCain, Graham, Scott Walker all got money from one single Russian.](https://www.dallasnews.com/opinion/commentary/2018/05/08/putins-proxies-helped-funnel-millions-gop-campaigns) n nMcConnell took $3.5 million n n&amp;gt; [Blavatnik contributed a total of $3.5 million to a PAC associated with Senate Majority Leader Mitch McConnell, R-Ky. Blavatnik contributed $1.5 million to the GOP Senate Leadership Fund PAC in the name of *Access Industries* and another $1 million in the name of *AI-Altep Holdings* during the 2015/2016 election season. And as of September 2017, he had contributed another $1 million this year through *AI–Altep.*](https://mavenroundtable.io/theintellectualist/news/mcconnell-received-3-5m-in-campaign-donations-from-russian-oligarch-linked-firm-93UjehU6aUCtejJRBFezCw/) n n*** n[Michael Cohen was the Deputy Finance Chairman of the RNC](https://www.businessinsider.com/trump-lawyer-michael-cohen-rnc-finance-executive-2017-4) n*** nA confirmed [Russian spy was moving money through the NRA to politicians](https://www.theguardian.com/us-news/2018/dec/10/maria-butina-russian-agent-nra-kremlin-infiltrate-plead-guilty) n*** n[All Republicans in the senate, a large amount of Republicans in the house and a 4 Dems in the house](https://www.opensecrets.org/orgs/recips.php?id=d000000082&amp;amp;cycle=2016) took money from the NRA n n n n*** n n[The NRA and the Trump campaign were illegally coordinating ad buys.](http://fortune.com/2018/12/07/trump-campaign-and-nra-illegally-coordinated-during-presidential-election-watchdog-groups-say/) n*** n n[NRA May Have Illegally Coordinated With GOP Senate Campaigns](https://truthout.org/articles/nra-may-have-illegally-coordinated-with-gop-senate-campaigns/) n*** n n nThere is much much more, but it&#39;s everywhere. n nThe thing is, with all this information out there. All that the Russians needed to get by hacking the GOP was that they knew that the money was coming from Russia (illegally) to the NRA and they have everyone on the hook for knowingly taking that money. What are your odds that the people who can&#39;t figure out how to format a PDF were talking about it in emails or other tech? n n*** n*** n*** n nEXAMPLE: n n[Graham before December 2016](https://www.youtube.com/watch?v=3SmM_N4Zy_U) n&amp;gt; [Trump] He is a jackass... and he shouldn&#39;t be Commander in Chief n n*** n[Graham after December 2016](https://www.politico.com/story/2017/04/lindsey-graham-praises-trump-237361) n&amp;gt; I am like the happiest dude in America right now,” a beaming Graham said on “Fox &amp;amp; Friends.” “We have got a president and a national security team that I’ve been dreaming of for eight years. n n*** n nWhat happened between? - [Graham: Russians hacked my campaign email account](https://www.cnn.com/2016/12/14/politics/lindsey-graham-hacking-russia-donald-trump/index.html) n*** n[And somewhere in there he took Russian money.](https://www.dallasnews.com/opinion/commentary/2018/05/08/putins-proxies-helped-funnel-millions-gop-campaigns)&#34;, &#34;Yes, Russians are spreading propaganda, but let&#39;s put the blame where it belongs. Even though ~~Dr.~~ Mr. Andrew Wakefield retracted his findings he takes a bulk of the responsibility and that Jenny McCarthy for using her fame as a crutch to spread that disinformation on a national platform n nEdit *it&#39;s been brought to my attention that Wakefield still stands by his BS it was the journal that published his results that retracted the paper. &#34;, &#39;To quote Doughboy from Boyz n the Hood, most Americans &#34;...don &#39;t know, don &#39;t show, or don &#39;t care.&#34; n nYou could put the evidence right in front of their eyes and show them the pile of 200,000 civilians corpses murdered, the CIA drug money that funded it, the Washington politicians fingerprints, etc., and they &#39;re still just shrug and not care. n&#39;, &#34;&amp;gt;Replying to Blaze on Twitter, Ocasio-Cortez wrote: “Thanks! Bartending + waitressing (especially in NYC) means you talk to 1000s of people over the years. **Forces you to get great at reading people + hones a razor-sharp BS detector.** n nIt&#39;s proof that talent is distributed evenly; it&#39;s opportunity that&#39;s hoarded. AOC broke through the wall and why the establishment cannot figure her out n&#34;, &#34;GOP Congressmen using mob lawyer&#39;s tactic when one of the mobsters testifies against the mob leader: n n&amp;gt;Well he&#39;s a criminal, we can&#39;t trust him. n nNo shit. Everybody is a criminal involved with the mob boss. That&#39;s why we have oaths and perjury laws.&#34;, &#39;Brilliant. I’m totally sold on AOC. I’m not gonna lie. Now, this support can easily be taken away if a few years from now, she’s “all talk and no action” but I just don’t get that vibe from her. She plans on getting shit done. n nI said years ago that nothing would really start to change until millennials got old enough to start getting into Congress. I feel validated watching AOC, and her policies, gain popularity in spades. &#39;, &#39;So they have been sweeping 1,000+ rape/sexual assault accusations under the rug every year since 2015. I’m sure before that also. This has to stop. They need to completely reform the staffing and policies at these detainment centers. Also this is just children. I imagine there are pretty high numbers of adults also sexually assaulted. How many people have to go through this torture for them to do something about it ?&#39;, &#39;First? n nEdit: Woo! Here’s a meme I made to celebrate the occasion. [Melania Invites](http://imgur.com/E831lfm)&#39;, &#39;Half of Texas’s new migration from other states comes from Cali, something that has been steadily turning the state bluer by the year. Texas GOP has been wailing about this for 15 years, they see it coming and there isn’t a damn thing they can do about it. n nMeanwhile, CA is so fucking blue and with a population of 40 million it could give up enough democratic voting residents to turn Texas, Florida and Ohio blue forever and still have blue votes to spare. &#39;, &#39;Another difference is king says this stupid racist shit DAILY and defends it, while northam did some stupid racist shit in college 35 years ago and tries to deny/distance himself from it. And we still want northam out, while gopers wanna keep king. smh.&#39;, &#39;AOC just slam dunked so hard she broke the backboard &#39;, &#34;I found the disinformation about this subject incredibly disheartening. I&#39;m glad that Politifact got this out there. n nThis new law is basically bringing an antique pre-Roe-v-Wade New York State Law into agreement with federal guidelines on how to deal with dead or dying fetuses. n nIf people want to have a debate fine... but for gods sake, debate with facts, not with disinformation, hatemongering, and scare tactics.&#34;, &#34;I respect Obama, I admire Obama. I&#39;m proud that I voted for him twice. n nBut if Obama blindly believed Putin over our own intelligence and the intelligence of our allies I would turn on Obama so fucking hard. I would be straight up calling for Obama impeachment. n nAnd you see folks, thats different between normal people and Trump supporters&#34;, &#34;Bernie’s one of my top choices, but just remember everyone, the priority is getting behind whichever Democrat wins the primary and getting that obese, orange, criminal piece of shit out of office. n nThere will be an effort to polarize us between Democratic candidates just like last time, but we need to stay unified no matter what. n nEdit: To clarify for some, vote for whoever you want in the primary. But in the general, yes, I am saying to vote blue no matter who, and that&#39;s nothin&#39; to be ashamed about when our opposition is the Republican Party. ;) *thumbs up*&#34;, &#34;Turns out when you put a bunch of people through a rat race that has no cheese at the end, don&#39;t be surprised when they stop and want to burn the whole maze down ... signed, a burnt out millennial &#34;, &#39;He has always been trying to compete with Obama, it is pretty pathetic. He does surpass Obama in a number of different ways though. He has more bankruptcies, he has cheated on his wife more than Obama, he has lied more, played golf more, hurt the US role in the community, and numerous other ways. So yeah, he is much better at Obama when it comes to those sort of things. &#39;, &#34;No it&#39;s brilliant! First we get rid of everything energy efficient, then the renewable power wont be enough, and we&#39;ll come crawling back to coal!&#34;, &#39;Saying stupid racist shit *is* free speech. Then non-idiots have the right to drown him out or kick him out of private property. n nThreats aren’t free speech n nFlashing a gun is brandishing and he should be charged. &#39;, &#39;Elijah Cummings is exactly the right man for the role he is playing today. God bless that patriot. &#39;, &#39;It &#39;s a classic &#34;criminal filibuster,&#34; where you commit so much crime that the prosecuter cannot live long enough to write the report on you.&#39;, &#39;&amp;gt; Sanders has repeatedly called climate change “the single greatest threat facing our planet,” and in 2016 campaigned on cutting carbon pollution by 40 percent by 2013, in part through an aggressive carbon tax on pollution. n nIn 2016, Sanders was the only candidate to support a carbon tax (though [Gary Johnson had briefly done so](https://thehill.com/policy/energy-environment/292308-libertarian-candidate-backs-carbon-fee), before cowardly [walking it back](https://reason.com/blog/2016/08/26/gary-johnson-no-to-carbon-taxes-and-mand)). n nThis time around, several candidates support a carbon tax (including [Delaney and Gillibrand](https://thehill.com/policy/energy-environment/429342-what-key-2020-candidates-are-saying-about-the-green-new-deal)) as [record numbers of Americans are alarmed about climate change](http://climatecommunication.yale.edu/publications/americans-are-increasingly-alarmed-about-global-warming/), the [Environmental Voter Project](https://www.environmentalvoter.org/) works to [increase turnout of environmental voters](https://www.youtube.com/watch?v=yCL1luiOM7U&amp;amp;t=2m53s), and majorities of Americans in [each political party](http://climatecommunication.yale.edu/wp-content/uploads/2018/05/Global-Warming-Policy-Politics-March-2018.pdf) and [every Congressional district](http://climatecommunication.yale.edu/visualizations-data/ycom-us-2018/?est=reducetax&amp;amp;type=value&amp;amp;geo=cd) supports a carbon tax. n nThe consensus among [scientists](http://bush.tamu.edu/istpp/scholarship/journals/ClimateScientistsPerspectives_ClimaticChange.pdf) and [economists](http://policyintegrity.org/files/publications/ExpertConsensusReport.pdf) on [carbon taxes](https://en.wikipedia.org/wiki/Carbon_tax) to mitigate climate change is similar to [the consensus among climatologists](http://climate.nasa.gov/scientific-consensus/) that human activity is responsible for global warming.&#39;, &#39;&amp;gt; tHer dad and brother may be huge racists but that doesn’t mean that @IvankaTrump can’t use “black history month” as an opportunity to launder her brand. n nMy personal favorite. n nEdit: I lied. n n&amp;gt; tWhere does Birther theory fit into this picture sweetie[?]&#39;, &#39;Love to read replies from people who don’t understand that every office gets a set amount of money and that AOC isn’t spending more, just not paying senior staff more than $80,000. &#39;, &#39;Fuck that fucking guy. n nFake hate crime accusations are like fake rape accusations. They give the accuser a moment in the spotlight, it destroys an innocent person &#39;s life and it sucks away credibility from actual victims of hate crimes and assaults. n nAlso I &#39;m seeing a lot of hypocracy from this sub over this whole issue, brushing this off as &#34;mental illness&#34; and shit. No, that asshole knew what he was doing. He knew the repercussions of it. He just didnt expect to be caught. Stop defending this crap. &#39;, &#39;&#34;How can we believe anything you say? The answer is, &#39;We can &#39;t &#39;&#34; n nUm yes you can because Cohen is literally providing proof? How fucking stupid is congresswoman Miller?&#39;, &#39;Also zero talk about further Russian sanctions for violating the nuke treaty. Just more Iranian missile program nonsense.&#39;, &#39;If he’s going to lie about it, just make up a number like “eleventy scrabillion dollars!” n nEdit: Wow, thank you for the gold, kind stranger!&#39;, &#39;Two days ago, a sub on Reddit was warning its users that they are under attack by liberals/democrats, and that liberals/democrats aren &#39;t going to stop until conservatives are &#34;destroyed&#34; - with additional calls to arm themselves. It garnered over 7,000 upvotes. The comments further claimed that liberals/democrats want to take guns from conservatives and instead, give them to &#34;illegals.&#34; n nI am afraid this is just the tip of the iceberg.&#39;, &#34;The Republican response to this is sooo fucking stupid politically. Its hilarious to watch. The tax law was at 46% approval at the midterms according to Gallup. That&#39;s back when lots of these people thought they were getting that bigger paycheck AND their tax return in tact - actually many were expecting an even bigger return still. So when they find out that&#39;s not true, Republicans respond by effectively calling them stupid for actually thinking the tax law was going to be better than this for them. 46% approval at the midterms and dropping rapidly.&#34;, &#34;And this is why I&#39;m proud to vote straight Democrat tickets. We don&#39;t need to increase the pregnancy mortality rate because some organized rape ring thinks that they have a sense of morality. &#34;, &#39;[Michael Weiss is breaking it down on twitter.](https://twitter.com/michaeldweiss/status/1100625911248957441) n n&amp;gt;Important. Buzzfeed reported that Trump instructed Cohen to lie to Congress. Cohen says Trump didn’t explicitly tell him to lie but that Trump instructed him to do so “in his own way.” This is how the reporting got muddled and why Mueller demurred. nSo this is why Mueller pushed back on Buzzfeed. It was a matter of Cohen’s interpretation of what Trump was asking; but not a direct order to lie: nhttps://twitter.com/michaeldweiss/status/1100623794522152960/photo/2&#39;, &#34;This was incredible. Not only what we the public have learned, but it is so damn refreshing seeing democrats able to actually do their job and serve the people of America, all the while handing verbal beat downs to their republican colleagues... I&#39;m guilty of cherishing those. n nBut I have to say the most impressive display for me was Alexandria Ocasio-Cortez. She was so well prepared and executed her line of questioning flawlessly, without any mind for rhetoric. She meant business and commanded the attention of someone who could be a future president.&#34;, &#39;I feel like compared to the current GOP platform and people who support Thee Donald giving tax dollars to a school makes you a raging socialist. n nSo yeah us Libs probably seem pretty left now, but the right has abandoned any notion of a middle ground so it’s natural that the other half the country follows suit &#39;, &#39;&amp;gt;Since he took office, Trump has appointed at least eight people who identified themselves as current or former members of [Mar-a-Lago] to senior posts in his administration. n nPay to play, folk$.&#39;, &#34;A lot of the information that is sealed and redacted is related to Russian collusion because of the sheer explosiveness if such information were found by the media. For example, when Manafort&#39;s lawyers forgot to redact that Manafort passed polling information to the Russians (with that polling information, the Russians then knew who and where to target on social media). n nI can&#39;t say that this specific incident of the judge clearing the courtroom is related to Russian collusion. But if I were to make an educated guess, I wouldn&#39;t be surprised if it is related to Russian collusion either, knowing what Manafort&#39;s lawyers redacted before.&#34;, &#39;It’s very... strange to hear these old millionaire politicians chanting “USA” like this is some sport to them. n n&#39;, &#39;Wow! I’m so glad we gave the rich that massive tax cut! They’re definitely using it to hire workers! /s&#39;, &#39;Why does media still treat him like a real journalist? n nBesides that he hides behind the alex jones entertainment excuse, he literally campaigned for Trump. n nNobody should ever know, or report on anything Hannity does. The only way you should hear what he says or thinks is if you tune directly into Fux News. I hate that. n nIf someone like Lil Wayne, or Jay-Z, was to take a jab at Trump he &#39;d be told to stick to entertaining. But they are bigger and better entertainers than Hannity, so shouldn &#39;t they have &#34;News&#34; shows too?&#39;, &#34;Does Reddit, too? n nReddit lets Putin&#39;s disinfo bots run wild.&#34;, &#34;A lot of hate in this thread. It&#39;s no coincidence that Bernie&#39;s 2016 campaign has set the stage for the 2020 DNC Election. Don&#39;t give in to the rhetoric. Push back and fight on. Feel the Bern&#34;, &#34;Get your fucking kids vaccinated. It&#39;s irresponsible to people who cannot get them and rely on herd immunity&#34;, &#39;She broke it down so simple that anyone with a few brain cells can understand. n nI doubt anyone could get lost in what she was getting at. Plain english is so effective. Perhaps other lawmakers should try to be this honest and down to earth?&#39;, &#39;Anyone crying &#34;B-b-b-b-but Obama did it!&#34; needs to take a good hard look at themselves. National emergencies aren &#39;t inherently bad. This, however, is an obese con stroking his own ego and nothing more, even by his own admission. n nIf you &#39;re gonna troll, get a better argument.&#39;, &#34;Do a search on politics or news and everything&#39;s been deleted except this single post. They kill anything that does not fit their leftist narrative and holy smokes anything that shits on the right gets tens of thousands of upvotes. &#34;, &#34;I will never forget watching a bunch of old white politicians - the true Good Ole Boys Club - laughing and joking around after Dr. Ford&#39;s hearing. n nThey never intended to help her. It was all about helping him. The fuckwads.&#34;, &#39;I just don’t get trump’s motivation. He’s in his 70s and in shitty shape. What does he have? A few more years on this mortal coil? He’s trading the possibility to become a dictator for those few years for the certainty of being in the history books as a monster. Who makes that calculation and decides to go with it?&#39;, &#39;Elizabeth Warren lied about being Native American to get into Harvard.&#39;, &#39;I listen to too much conservative talk radio. In one 5 minute span, Rush Limbaugh today talked about two issues. n nThe first was a statement by FBI General Counsel James Baker, whose comments in a closed-door testimony were recently released to the public. Speaking about Hillary &#39;s private server, Baker said &#34;[he] initially thought Clinton’s behavior was ‘alarming’ and ‘appalling &#39;” but late in the process was talked out of pursuing charges. n nThe second was about FBI Director Andrew McCabe, who considered trying to get the Cabinet to consider the 25th Amendment but was talked out of pursuing the matter. n nAccording to Limbaugh, the first FBI member to get talked out of pursuing further action against a political figure is proof that Hillary should have gone to jail. The second FBI to get talked out of pursuing further action is proof of a silent coup which should see its originator arrested and in solitary confinement. n nLimbaugh: n&amp;gt;So, A, that doesn’t apply. B, “no reasonable prosecutor” would prosecute? Well, it turns out that a reasonable prosecutor in the FBI did want to prosecute her and had to be talked out of it by McCabe and Comey. n nAlso Limbaugh: n n&amp;gt;Apparently, McCabe was out of control. I think Andrew McCabe is a very bad man. I mean, just speaking as a straight-up-and-down observation, he is a very bad man. Not only was this guy plotting a bureaucrat coup after Comey was fired — and that coup was to protect Comey’s conspirators. He was trying to overthrow the presidential election. n nWhat is the difference here? Not charging the Democrat is proof of a conspiracy by Democrats to cheat the election. Not charging the Republican is proof of a conspiracy by Democrats to cheat the election. n nNo, he didn &#39;t see the irony *or* the hypocrisy.&#39;, &#34;AOC wants those tax returns. Lol. She&#39;s gonna get them too.&#34;, &#39;“The only thing these people did wrong was believe the EPA when they said the air is safe. They were lied to by our government. Then they were promised this compensation, and now the rug is being pulled out from underneath them,” said attorney Michael Barasch, who represents many 9/11 illness victims.&#39;, &#34;Why should she move, she didn&#39;t ask to be raped? Isn&#39;t it better to prevent additional rapes by moving her rapist into a prison cell?&#34;, &#39;Hopefully this time when mcsalley loses she won’t become a senator &#39;, &#39;Seriously people? Supporting a sitting president which 45%-50% of the country does, is the same as supporting the KKK? n nYou upvote and accept this story, while downvoting and removing the Jussie Smollett hate crime hoax? n nedit: Okay people, support and approve are two different things. Seems like some people enjoy saying &#34;omg his approval is not 45% it &#39;s actually only 43.5%&#34; ... it &#39;s a minor detail.&#39;, &#39;A tweet for everything n nhttps://mobile.twitter.com/realDonaldTrump/status/535441553079431168 n n&amp;gt; Repubs must not allow Pres Obama to subvert the Constitution of the US for his own benefit &amp;amp; because he is unable to negotiate w/ Congress&#39;, &#39;Why the **FUCK** does CNN give that shit stain air time? n nPA resident here, FUCK YOU SANTORUM, FUCK YOU TOOMEY&#39;, &#39;Plaskett is killing it. Wonderful follow up to that clown Chip. Shows what party cares about this country and what party cares about itself. &#39;, &#34;Just like they subpoenaed Trump&#39;s taxes. This government has taught us subpoenas mean shit when they&#39;re served to the privileged :(&#34;, &#39;He stated *&#34;I want to see the American people win. I want to see America win. I don &#39;t care if you &#39;re a Democrat, independent, libertarian, Republican. Bring me your ideas,” Shultz said. “And I will be an independent person who will embrace those ideas. Because I am not, in any way, in bed with a party.”* n nThat &#39;s....that &#39;s not how it works. It is your job to present your ideas and policies to us. And then it &#39;s our job to decide whether those ideas merit our vote for you. n nThis strange milquetoast statement by this basic billionaire just shows how vapid he really is. He has no ideas. He has no visions. He only has platitudes, platitudes as deep as &#34;Live, Laugh, Love&#34;. And if the media is not going to ignore him they need to point out his pathetic responses. &#39;, &#34;It isn&#39;t just Trump. n n[McConnell, Rubio, McCain, Graham, Scott Walker all got money from one single Russian.](https://www.dallasnews.com/opinion/commentary/2018/05/08/putins-proxies-helped-funnel-millions-gop-campaigns) n nMcConnell took $3.5 million n n&amp;gt; [Blavatnik contributed a total of $3.5 million to a PAC associated with Senate Majority Leader Mitch McConnell, R-Ky. Blavatnik contributed $1.5 million to the GOP Senate Leadership Fund PAC in the name of *Access Industries* and another $1 million in the name of *AI-Altep Holdings* during the 2015/2016 election season. And as of September 2017, he had contributed another $1 million this year through *AI–Altep.*](https://mavenroundtable.io/theintellectualist/news/mcconnell-received-3-5m-in-campaign-donations-from-russian-oligarch-linked-firm-93UjehU6aUCtejJRBFezCw/) n n*** n[Michael Cohen was the Deputy Finance Chairman of the RNC](https://www.businessinsider.com/trump-lawyer-michael-cohen-rnc-finance-executive-2017-4) n*** nA confirmed [Russian spy was moving money through the NRA to politicians](https://www.theguardian.com/us-news/2018/dec/10/maria-butina-russian-agent-nra-kremlin-infiltrate-plead-guilty) n*** n[All Republicans in the senate, a large amount of Republicans in the house and a 4 Dems in the house](https://www.opensecrets.org/orgs/recips.php?id=d000000082&amp;amp;cycle=2016) took money from the NRA n n n n*** n n[The NRA and the Trump campaign were illegally coordinating ad buys.](http://fortune.com/2018/12/07/trump-campaign-and-nra-illegally-coordinated-during-presidential-election-watchdog-groups-say/) n*** n n[NRA May Have Illegally Coordinated With GOP Senate Campaigns](https://truthout.org/articles/nra-may-have-illegally-coordinated-with-gop-senate-campaigns/) n*** n n nThere is much much more, but it&#39;s everywhere. n nThe thing is, with all this information out there. All that the Russians needed to get by hacking the GOP was that they knew that the money was coming from Russia (illegally) to the NRA and they have everyone on the hook for knowingly taking that money. What are your odds that the people who can&#39;t figure out how to format a PDF were talking about it in emails or other tech? n n*** n*** n*** n nEXAMPLE: n n[Graham before December 2016](https://www.youtube.com/watch?v=3SmM_N4Zy_U) n&amp;gt; [Trump] He is a jackass... and he shouldn&#39;t be Commander in Chief n n*** n[Graham after December 2016](https://www.politico.com/story/2017/04/lindsey-graham-praises-trump-237361) n&amp;gt; I am like the happiest dude in America right now,” a beaming Graham said on “Fox &amp;amp; Friends.” “We have got a president and a national security team that I’ve been dreaming of for eight years. n n*** n nWhat happened between? - [Graham: Russians hacked my campaign email account](https://www.cnn.com/2016/12/14/politics/lindsey-graham-hacking-russia-donald-trump/index.html) n*** n[And somewhere in there he took Russian money.](https://www.dallasnews.com/opinion/commentary/2018/05/08/putins-proxies-helped-funnel-millions-gop-campaigns)&#34;, &#39;Relevant piece from *Lawfare* last month: [&#34;What if the Obstruction Was the Collusion? On the New York Times’s Latest Bombshell&#34;](https://www.lawfareblog.com/what-if-obstruction-was-collusion-new-york-timess-latest-bombshell) n nBy dangling pardons, Trump isn &#39;t just obstructing justice to cover up his own crimes, he is doing it to protect Russia and Vladimir Putin. Undermining all investigations into Russia &#39;s activities, including his own campaign &#39;s collusion with them, are part and parcel of his broad subordination to Russia, right alongside his efforts to reduce and slow walk sanctions.&#39;, &#34;* She&#39;s young n n* She&#39;s intelligent n n* She&#39;s outspoken and easy to understand n n* Her ideas are hugely popular and resonate with the majority of Americans n n* She&#39;s progressive n n* She&#39;s not white n n* She takes no shit n n* She gets modern technology and makes the regressive old farts feel obsolete n nShe checks all the boxes of things the GOP hates. It&#39;s no wonder they&#39;re obsessed with her.&#34;, &#39;I know it’s beneath them and wouldn’t end well, but I really want every single Democrat in unison to yell “you lie!“ every time Trump lies. Treat the fucker like they treated Obama. You know he would flip out.&#39;, &#34;See that thing that&#39;s holding Obama up so high? That&#39;s called a spine. I miss having a president with one of those.&#34;, &#34;Tax the shit out of these plutocrats in marginal. I&#39;m done. I will never support low taxation for rich people, ever again. n nOh, and 100% estate tax too. End generational Plutocracy. n&#34;, &#34;Remember when Obama asked the FBI about ongoing investigations and who they would appoint to oversee certain cases? No? Because he was hands off, kept out of all investigations including all the ones against Hillary, rarely even commented about it. That is the way it is supposed to be, you find the best people to run the agency, then you let them do their jobs, you don&#39;t interfere.&#34;, &#34;So frustrating. n nThe president&#39;s lawyer is arrested. n nThe president&#39;s lawyer turns on the president. n nThe president&#39;s lawyer implicates the president in multiple crimes. n nEVERY president supporter is convinced the president&#39;s lawyer is nothing but a giant lawyer. n nSo frustrating.&#34;, &#34;Evidence of a political master at work. This is why many of us said Pelosi deserved the gavel again. n nFor those of you that don&#39;t get this, let&#39;s spell it out. n n1) There&#39;s nothing she can really do to prevent it. In fact, she knows that her posturing on this makes it *more likely* Trump will do it again. This is baiting him to do it. *Why?* She knows the last shutdown was a big win for Dems and disaster for Trump, she *doesn&#39;t mind* another shutdown. A shutdown is very good for the Dems politically. You have to understand this point to understand why she can say this. Pelosi wins either way. n n2) If Trump pulls another shutdown: Pelosi and Dems laugh and enjoy the continuing implosion of Trump. If he doesn&#39;t shutdown again, Pelosi looks dominant and rubs in her superior positioning and political skill. Trump is the consummate bully, he only understands power and dominance--Pelosi has his number. n n&#34;, &#39;I heard this while I was getting ready this morning and the host really went after her hypocrisy beyond this issue as well. It was great. &#39;, &#34;As the descendent of a Confederate soldier, I&#39;d like to point out that that flag represents the heritage of racists and losers. Losers who can&#39;t get over a loss that occured over 100 years ago and seek to oppress the votes of people just like their loser predecessors. Mitch and that flag belong in one place: a museum where they can be ignored by bored 6th graders. n nObligatory Gilded Edit: Who would have thought a random angry post would&#39;ve blown up so much. Thanks stranger for the gold!&#34;, &#34;Ted Cruz may be the least pleasant human being alive who isn&#39;t an actual serial killer/sexual predator type &#34;, &#39;It’s really interesting how mainstream positions such as a $15 minimum wage and Medicare for all have already become. I like Bernie but I’ll support any Democrat who gets the nomination.&#39;, &#39;Can’t wait until Democrats do the same thing for healthcare and gun control. &#39;, &#39;Here is another analogy. In the 80s OPEC decided to limit oil production. Oil producing countries such as Saudi Arabia thought that less oil would increase the price of oil. I know because I worked for an oil company and the company experts said oil would reach $400.00 a barrel. For a time oil did go way up in price but then an interesting thing happened. People bought more fuel efficient cars, non OPEC countries found more oil, people found alternate forms of energy. Eventually the world became more fuel efficient and oil prices decreased much to the irritation of OPEC. n nHow does this relate to China buying soybeans and other products from the US? Well, farmers think the Chinese will come groveling back to Trump and say so sorry we will pay top dollar for US products. However what we see happening is that China is turning to alternatives. They are buying products from other countries. They are changing what products they need to buy, they are also investing in other countries so they don’t need us. In a way I don’t blame them. nThe US does have some legitimate complaints about China such as intellectual rights, open markets and so on. However you don’t remove an inflamed appendix with a sledgehammer but rather a scalpel, In the same way we need a president who can wield a scalpel toward China trade and trade with other countries and Trump is not that president. &#39;, &#39;Good, now when a dem gets elected, they can declare a national emergency on vaccinations (force immunization), opiods, guns, national debt (raise taxes)...&#39;, &#39;I thought the far left was an angry mob. Why would anyone want to threaten an angry mob leader? These are trump supporters though so I guess stupidity is in character.&#39;, &#39;Cohen: &#34;When Mr Trump turned around early in the campaign and said he could shoot someone on 5th Ave... he &#39;s not joking. He &#39;s telling you the truth.&#34; n nEdit: n nWhy this matters: [Trump has threatened Cohen &#39;s family.](https://www.vox.com/2019/1/23/18194719/trump-michael-cohen-father-in-law-threat) &#39;, &#34;“Once again, however, Trump’s firehose of news disintegrates into white noise (pun intended) while relatively minor Democratic scandals are amplified -- because there are so few by comparison and therefore each thing is easier to remember individually.” n nThis is exactly the issue on top of “the Republicans and even Trump himself pile onto the Virginia Democrats as if Fox News and the GOP hadn&#39;t gaslighted the world by shrugging off Trump’s trespasses as “locker room talk” or some other form of non-threatening gaffe.”&#34;, &#39;Wouldn &#39;t be a problem, except that a. he &#39;s president *, b. he wants the role of the guy with the little mustache, c. he &#39;s convinced [*with coordinated help from resourceful, dishonest parties* ] 40 million other people that his fantasy is also grounded in reality. Um... n nIt &#39;s OUR job to do sanity checks for people who can &#39;t or refuse to; extra effort is required regarding those who consciously aim to &#34;infect&#34; others with their bullshit for the purposes of manipulation and advantage. n n*Advertising often approaches or exceeds this. Maybe it &#39;s just what we &#39;re used to, or have become, as a consequence of being such a consumption driven society?That* ***beautiful lie*** *of the perfect sandwich or drug or toy or seed or fuel. We know better, right? Kids do not. Untrained, lazy, and incompetent are effectively kids. Add expensive, competent [sometimes illegal ] &#39;campaigns &#39; to this and you have a social* ***weapon***. *Just because Germany was defeated in 1945, the &#34;war&#34; isn &#39;t over for people who rejected or failed to get the news (where typically, responsible authorities would step in &amp;lt;****you****, Mitch McConnell, GOP in congress; y &#39;all really fucked up this time. maybe ignorance is bliss?&amp;gt;).*&#39;, &#39;[poppinkreamjewels](https://np.reddit.com/r/politics/comments/9r3vym/cnn_to_trump_you_incited_this/e8e0l9d/) n n*** n n**President Trump has incited violence against his political opponents innumerable times.^[[1]](https://www.youtube.com/watch?v=WIs2L2nUL-0)** n nHalf a dozen of the President &#39;s so called &#34;enemies&#34; were targeted and explosive devices were sent to their offices or residences.^[[2]](https://www.foxnews.com/politics/suspicious-package-found-at-clintons-home-police-say) n n* Former CIA Director^[[3]](https://www.cnn.com/2018/08/16/politics/donald-trump-brennan-security-clearance/index.html) John Brennan^[[4]](https://www.nbcnews.com/politics/white-house/trump-says-rigged-witch-hunt-prompted-him-revoke-brennan-s-n901626) sent to CNN - President Trump has called the media &#34;The enemy of the people&#34;^[[5]](https://www.npr.org/2018/08/04/635461307/opinion-calling-the-press-the-enemy-of-the-people-is-a-menacing-move) n n n* President Bill Clinton^[[6]](https://www.nytimes.com/2016/10/01/us/politics/donald-trump-interview-bill-hillary-clinton.html) and Hillary Clinton^[[7]](https://www.nbcnews.com/politics/politics-news/trump-accuses-hillary-clinton-colluding-russia-crowd-chants-lock-her-n918836) going so far as to suggest deadly violence^[[8]](https://www.nytimes.com/2016/08/10/us/politics/donald-trump-hillary-clinton.html) n n n* George Soros^[[9]](https://www.washingtonpost.com/blogs/post-partisan/wp/2018/10/10/why-trump-and-the-republicans-keep-talking-about-george-soros/?utm_term=.e1a89320752b) n n n* President Obama^[[10]](https://www.nytimes.com/2018/02/21/us/politics/trump-attacks-obama-and-his-own-attorney-general-over-russia-inquiry.html) n n n* Former Attorney General Eric Holder^[[11]](https://www.axios.com/eric-holder-donald-trump-be-careful-2020-election-20242a69-8cb0-4e5c-9ea5-f356ff2869a4.html) n n n* Congresswoman Maxine Waters^[[12]](https://www.theguardian.com/us-news/2018/jul/07/maxine-waters-trump-progressives-california-congress) n n n**The President &#39;s attacks against political opponents, the free press and praise for dictators** n nThe rhetoric and actions taken by the President - from continuing to berate the fourth estate by referring to the media as &#34;fake news&#34;^[[13]](https://www.washingtonpost.com/news/the-fix/wp/2018/05/22/trump-admitted-he-attacks-press-to-shield-himself-from-negative-coverage-60-minutes-reporter-says/?utm_term=.5f603dabb50e) to calling his political opponents traitors^[[14]](https://www.theatlantic.com/politics/archive/2018/02/one-dares-call-it-treason/552395/) while he attacks the judicial branch of government without remorse,^[[15]](https://www.washingtonpost.com/news/the-fix/wp/2017/04/26/all-the-times-trump-personally-attacked-judges-and-why-his-tirades-are-worse-than-wrong/?utm_term=.a7a49c427ef6) are just a few examples of his egregious attacks on democratic institutions and norms. n nPresident Trump has referred to the minority party as un-American for not applauding his speech.^[[16]](https://www.usatoday.com/story/news/politics/2018/02/05/trump-blasts-treasonous-democrats-not-applauding-his-state-union-address/301962002/) President Trump joked about wanting to consolidate his power like his dictator colleague in China, President Xi.^[[17]](http://www.dw.com/en/us-president-donald-trump-praises-chinas-xi-jinping-for-consolidating-grip-on-power/a-42817441) President Trump has repeatedly praised dictators including Putin, Duterte, Erdogan, and el-Sisi.^[[18]](https://www.theatlantic.com/international/archive/2018/03/trump-xi-jinping-dictators/554810/) n n&amp;gt;Trump’s fondness for authoritarians may have more to do with how power is wielded than those who exercise it. It just so happens that Western governments have, for the past seven decades, mostly adhered to a system of the rule of law, which empowers institutions rather than individuals. Trump’s apparent preference is for a system in which one individual, presumably him, wields that power. n n&amp;gt;Indeed, his fondness for strongmen and dictators isn’t limited to Xi Jinping or any other individual in power now. He has praised Iraq’s Saddam Hussein (while also criticizing him as “a bad guy”) for killing terrorists. “He did that so good,” Trump said in July 2016. “They didn’t read them the rights. They didn’t talk. They were terrorists. Over.” n n&amp;gt;Trump also said in 2016 that Libya would be better off “if [Moammar] Gaddafi were in charge right now.” He once tweeted a quote from Benito Mussolini, the Italian fascist leader, and later defended the tweet, saying: “Mussolini was Mussolini ... It’s a very good quote. It’s a very interesting quote... what difference does it make whether it’s Mussolini or somebody else?” n n&amp;gt;Trump even said China’s brutal crackdown on protesters in Tiananmen Square in 1989 “shows you the power of strength,” contrasting the Communist Party’s action with the United States, which he said “is right now perceived as weak.” Trump made those comments in 1990. When asked about the remarks during the presidential debate in 2016, Trump defended himself and appeared to take the Chinese Communist Party’s view of the events at Tiananmen. He dismissed the deadly military response as a “riot.” n n&amp;gt; n nFollowing Saudi Arabia &#39;s grotesque assassination of Saudi journalist Jamal Khashoggi in Turkey^[[19]](https://np.reddit.com/r/worldnews/comments/9pszpp/australia_pulls_out_of_saudi_summit_over/e84cdp9/) President Trump encouraged assaulting reporters and journalists at a rally in Montana last week.^[[20]](https://www.washingtonpost.com/blogs/erik-wemple/wp/2018/10/19/president-trump-greenlights-assaults-on-reporters/?noredirect=on&amp;amp;utm_term=.f11a6eb8f78b) n n*** n n1) [YouTube - All the Times Trump Has Called for Violence at His Rallies](https://www.youtube.com/watch?v=WIs2L2nUL-0) n n2) [Fox News - Explosive devices mailed to Obama, Hillary Clinton, others prompt security scare](https://www.foxnews.com/politics/suspicious-package-found-at-clintons-home-police-say) n n3) [CNN - Trump blasts former CIA Director John Brennan as &#39;loudmouth, partisan, political hack &#39;](https://www.cnn.com/2018/08/16/politics/donald-trump-brennan-security-clearance/index.html) n n4) [NBC - Trump ties &#39;rigged witch hunt &#39; to decision to revoke Brennan &#39;s security clearance](https://www.nbcnews.com/politics/white-house/trump-says-rigged-witch-hunt-prompted-him-revoke-brennan-s-n901626) n n5) [NPR - Opinion: Calling The Press The Enemy Of The People Is A Menacing Move](https://www.npr.org/2018/08/04/635461307/opinion-calling-the-press-the-enemy-of-the-people-is-a-menacing-move) n n6) [New York Times - Donald Trump Opens New Line of Attack on Hillary Clinton: Her Marriage](https://www.nytimes.com/2016/10/01/us/politics/donald-trump-interview-bill-hillary-clinton.html) n n7) [NBC - Trump accuses Hillary Clinton of colluding with Russia as crowd chants &#39;lock her up &#39;](https://www.nbcnews.com/politics/politics-news/trump-accuses-hillary-clinton-colluding-russia-crowd-chants-lock-her-n918836) n n8) [New York Times - Donald Trump Suggests ‘Second Amendment People’ Could Act Against Hillary Clinton](https://www.nytimes.com/2016/08/10/us/politics/donald-trump-hillary-clinton.html) n n9) [Washington Post - Why Trump and the Republicans keep talking about George Soros](https://www.washingtonpost.com/blogs/post-partisan/wp/2018/10/10/why-trump-and-the-republicans-keep-talking-about-george-soros/?utm_term=.e1a89320752b) n n10) [New York Times - Trump Attacks Obama, and His Own Attorney General, Over Russia Inquiry](https://www.nytimes.com/2018/02/21/us/politics/trump-attacks-obama-and-his-own-attorney-general-over-russia-inquiry.html) n n11) [Axios - Trump says Eric Holder &#34;better be careful what he &#39;s wishing for&#34;](https://www.axios.com/eric-holder-donald-trump-be-careful-2020-election-20242a69-8cb0-4e5c-9ea5-f356ff2869a4.html) n n12) [The Guardian - &#39;You better shoot straight &#39;: how Maxine Waters became Trump &#39;s public enemy No 1](https://www.theguardian.com/us-news/2018/jul/07/maxine-waters-trump-progressives-california-congress) n n13) [Washington Post - Trump admitted he attacks press to shield himself from negative coverage, Lesley Stahl says](https://www.washingtonpost.com/news/the-fix/wp/2018/05/22/trump-admitted-he-attacks-press-to-shield-himself-from-negative-coverage-60-minutes-reporter-says/?utm_term=.5f603dabb50e) n n14) [The Atlantic - He Dares Call It Treason](https://www.theatlantic.com/politics/archive/2018/02/one-dares-call-it-treason/552395/) n n15) [Washington Post - All the times Trump personally attacked judges — and why his tirades are ‘worse than wrong’](https://www.washingtonpost.com/news/the-fix/wp/2017/04/26/all-the-times-trump-personally-attacked-judges-and-why-his-tirades-are-worse-than-wrong/?utm_term=.a7a49c427ef6) n n16) [USA Today - Trump blasts &#39;treasonous &#39; Democrats for not applauding at his State of the Union address](https://www.usatoday.com/story/news/politics/2018/02/05/trump-blasts-treasonous-democrats-not-applauding-his-state-union-address/301962002/) n n17) [Deutsche Welle - US President Donald Trump praises China &#39;s Xi Jinping for consolidating grip on power](http://www.dw.com/en/us-president-donald-trump-praises-chinas-xi-jinping-for-consolidating-grip-on-power/a-42817441) n n18) [The Atlantic - Nine Notorious Dictators, Nine Shout-Outs From Donald Trump](https://www.theatlantic.com/international/archive/2018/03/trump-xi-jinping-dictators/554810/) n n19) [PK - Saudi Arabia &#39;s assassination of a journalist and the world &#39;s response](https://np.reddit.com/r/worldnews/comments/9pszpp/australia_pulls_out_of_saudi_summit_over/e84cdp9/) n n20) [Washington Post - President Trump greenlights assaults on reporters](https://www.washingtonpost.com/blogs/erik-wemple/wp/2018/10/19/president-trump-greenlights-assaults-on-reporters/?noredirect=on&amp;amp;utm_term=.f11a6eb8f78b) n&#39;, &#34;[McConnell, Rubio, McCain, Graham, Scott Walker all got money from one single Russian.](https://www.dallasnews.com/opinion/commentary/2018/05/08/putins-proxies-helped-funnel-millions-gop-campaigns) n nMcConnell took $3.5 million n n&amp;gt; [Blavatnik contributed a total of $3.5 million to a PAC associated with Senate Majority Leader Mitch McConnell, R-Ky. Blavatnik contributed $1.5 million to the GOP Senate Leadership Fund PAC in the name of *Access Industries* and another $1 million in the name of *AI-Altep Holdings* during the 2015/2016 election season. And as of September 2017, he had contributed another $1 million this year through *AI–Altep.*](https://mavenroundtable.io/theintellectualist/news/mcconnell-received-3-5m-in-campaign-donations-from-russian-oligarch-linked-firm-93UjehU6aUCtejJRBFezCw/) n n*** n[Michael Cohen was the Deputy Finance Chairman of the RNC](https://www.businessinsider.com/trump-lawyer-michael-cohen-rnc-finance-executive-2017-4) n*** nA confirmed [Russian spy was moving money through the NRA to politicians](https://www.theguardian.com/us-news/2018/dec/10/maria-butina-russian-agent-nra-kremlin-infiltrate-plead-guilty) n*** n[All Republicans in the senate, a large amount of Republicans in the house and a 4 Dems in the house](https://www.opensecrets.org/orgs/recips.php?id=d000000082&amp;amp;cycle=2016) took money from the NRA n n n n*** n n[The NRA and the Trump campaign were illegally coordinating ad buys.](http://fortune.com/2018/12/07/trump-campaign-and-nra-illegally-coordinated-during-presidential-election-watchdog-groups-say/) n*** n n[NRA May Have Illegally Coordinated With GOP Senate Campaigns](https://truthout.org/articles/nra-may-have-illegally-coordinated-with-gop-senate-campaigns/) n*** n n nThere is much much more, but it&#39;s everywhere. n nThe thing is, with all this information out there. All that the Russians needed to get by hacking the GOP was that they knew that the money was coming from Russia (illegally) to the NRA and they have everyone on the hook for knowingly taking that money. What are your odds that the people who can&#39;t figure out how to format a PDF were talking about it in emails or other tech? n n*** n*** n*** n nEXAMPLE: n n[Graham before December 2016](https://www.youtube.com/watch?v=3SmM_N4Zy_U) n&amp;gt; [Trump] He is a jackass... and he shouldn&#39;t be Commander in Chief n n*** n[Graham after December 2016](https://www.politico.com/story/2017/04/lindsey-graham-praises-trump-237361) n&amp;gt; I am like the happiest dude in America right now,” a beaming Graham said on “Fox &amp;amp; Friends.” “We have got a president and a national security team that I’ve been dreaming of for eight years. n n*** n nWhat happened between? - [Graham: Russians hacked my campaign email account](https://www.cnn.com/2016/12/14/politics/lindsey-graham-hacking-russia-donald-trump/index.html) n*** n[And somewhere in there he took Russian money.](https://www.dallasnews.com/opinion/commentary/2018/05/08/putins-proxies-helped-funnel-millions-gop-campaigns)&#34;, &#39;AOC is all style, no substance. n nHer Green New Deal had ridiculous policies and was unbelievably expensive. She wants to eliminate nuclear energy use, despite it being the bedrock of any renewable energy plan. Estimates have put the Green New Deal at costing 13 TRILLION dollars. n nShe claimed people in NY were working 100 hours a week and not able to feed their families. Working 100 hours a week at minimum wage, as you get overtime pay over 40 hours, nets you 1,755 a week, or $91,260 a year. The median wage in the USA is $62,175 a year. n nHer 70% tax doesn’t affect the people who it’s aimed at. No one who earns over $10,000,000 can have it taxed as income. It’s capital gains, which she hasn’t made mention to at all. &#39;, &#34;&amp;gt; The reports show that DeVos is invested heavily in adoption agency corporations that have been receiving and adopting out the migrant children that are being separated from their parents at the border n npretty much trumpian motivation for appointments right there. don&#39;t even try to argue there&#39;s no pay to play.&#34;, &#34;Donald Trump Jr. retweeted this, demonstrating a lack of reading comprehension and highlighting the lack of judgment Cohen testified to: n n n&amp;gt;Incredible to see all the hubris drained from Cohen. I&#39;ve been personally screamed at by Cohen on the phone before and know how much bravado he once had. This is a man with nothing left, with no reason to lie or obfuscate at all. Humbling, in its way. n nhttps://twitter.com/vermontgmg/status/1100787323640397824&#34;, &#39;This fucking idiot literally spends 60% of his time watching television. n nImagine how your brain and thinking would look if you watched between 6-10 hours of foxnews a day. Thats what this inbecile does. n nIf you tally up the lunch time, and assume 30% of his travel/event time is campaign rallies, this means hes spending 75% of his time literally either at lunch, at a campaign rally, or watching tv.&#39;, &#34;Let&#39;s be honest, it&#39;s a complete joke that Elizabeth Warren claims to be Native American when she literally has one Native American ancestor going back between 6-10 generations ago. I literally have the same thing in my family, yet no one here is lying to themselves and saying their Native. n n&#34;, &#39;&amp;gt;It was revealed on Thursday that those phone calls actually involved NASCAR CEO Brian France and real estate developer/long-time Trump ally Howard Lorber, leading the president to declare that the meeting with Russians for “dirt” on Hillary Clinton was “innocent” after all. n nOf *course* NASCAR gets brought into this white-trash circus side show. Jesus, lol.&#39;, &#39;Hot take, whenever Repubs accuse the patriot left of anything, you can probably bet your bottom dollar that they are projecting their corruption. &#39;, &#39;I mean I live in Arizona, everything seems fine to me. If you talk to anyone I know here in Arizona, we would all agree we have no idea why border security is so high on the list for some Americans. I fucking love Mexico personally, and there really don’t seem to be many problems that’s Iv personally seen living here. So idk 🤷🏻 u200d♂️ I guess some people are just inherently scared of their told to be scared of lmao. n nEdit: wow guys thanks, I knew I wasn’t the only one who felt this way! n n&#39;, &#39;My boss and I have gotten nothing done today. I’ll hear him laugh or say “DAMN” from his office whenever shit hits the fan.&#39;, &#34;We really can&#39;t consider it a news story every time someone says Trump is racist. Ain&#39;t nobody got time for that.&#34;, &#39;Because the alternative policy is do nothing, let the sea levels rise, further ecological collapse. n nAnd have walls, drones, borders and an authoritarian police state to kill or enslave the refugees. When someone calls the Green New Deal unrealistic, and doesn’t have any better options. That’s what they’re implicitly arguing is a desirable outcome.&#39;, &#39;We stopped being a Democratic Republic when the Supreme Court awarded George W Bush the Presidency in 2000. &#39;, &#39;The same people who thought Obama was lazy support this guy. How does that make sense? &#39;, &#39;So let me get this straight. I’m supposed to be mad at the democrats for trying to put legislation forward to better working class Americans and the planet? This coming from the party that had full control of government for two years and couldn’t push their agenda if it hit them in the fucking head. Appeal Obama care. Nope. Build a wall. Nope. Ban Muslims and other brown people. Nope. n nLet’s not walk into the republican trap. Let’s go on the offensive. We produced a bill. Just because it doesn’t pass doesn’t mean it’s a failure. Failure is not trying. Therefore the republicans are the failures, up and until they can produce a comprehensive alternative that gets passed. n nThey can’t. So let’s make that the narrative. n n&#39;, &#39;&amp;gt; The Associated Press, however, reported that it will include only about $1.5 billion of the $5.7 billion that President Donald Trump has demanded for a wall. n nLol 1.5 is less than Dems originally offered before the first shutdown (1.6)&#39;, &#39;&amp;gt;Investigators found that from 1999 to 2005, Mr. Epstein, a former hedge fund manager with powerful friends, including **President Trump and former President Bill Clinton,** lured girls **as young as 14 or 15 years old** into his mansions in Palm Beach, New York and the Virgin Islands. n n&amp;gt;He paid them cash to engage in nude massages, masturbation and oral sex. In some instances, he asked girls to recruit other girls into his sex ring, the accusers told police. n n&amp;gt;Federal prosecutors had initially drafted a 53-page indictment against Mr. Epstein, but under the deal negotiated in 2008, he pleaded guilty to lesser state charges of soliciting a minor for prostitution and served 13 months at the Palm Beach County Stockade. n n**While there, Mr. Epstein was allowed to leave custody and work out of his office six days a week.** n nTLDR: Raping, trafficking, and pimping out hundreds of CHILDREN for over 6 years got him -13 months- in prison WHERE HE WAS RELEASED 6 DAYS A WEEK TO WORK AT HIS OFFICE.&#39;, &#34;Susan Collins lied her ass off. She gave her lying 45 minute speech about how Kavanaugh promised to protect Roe v Wade and a woman&#39;s right to choose and at the first opportunity, he proved he lied. He proved Collins lied. n nShe lied to Maine voters and now she&#39;s toast in 2020. &#34;, &#34;The Republicans kept accusing them of being liberal fake news, so now they&#39;re welcoming in Republicans to spew out fake news. n nRemember when CNN was legit unbiased? How their whole deal was just 24 hour news coverage? How did things get so bad&#34;, &#39;So he absolutely believes they did but can’t say because multiple members are Of the Trump admin are Saudis bitch... just like the Bush admin. n nHomeland Sec/FBI/CIA are still covering shit up about the Florida links to the family who housed a highjacker in Florida who went to flight school.&#39;, &#39;LOL wtf is this. n n&amp;amp;#x200B; n nWhat is up with Republican old white dudes just crying all the time in front of Congress? Jesus christ.&#39;, &#39;&#34;MAGA hats are the new white hood&#34; n nWelp, here &#39;s a Democrat in actual white hood&#39;, &#39;What kind of a person assumes a call to choose &#34;love over hate&#34; is a direct attack on them?&#39;, &#39;Yea he but once they know Democrats want it, Republican support will magically evaporate. We should let republicans think they thought of it. Call it the “George Soros/Clinton defundment act.”&#39;, &#34;I remember when the Clinton&#39;s left the White House. Bush&#39;s transition team claimed Hillary took furniture and left the place a mess. Wait till Trump leaves. It&#39;s going to be the biggest dine and dash in history. He&#39;s going to leave everything jammed up, unpaid, and owed. &#34;, &#39;&amp;gt;Washington Posts @RoigFranzia says Bezos &#39; team thinks it &#39;s possible that the text leaks were politically motivated and that a &#34;government entity&#34; accessed the Bezos texts n nhttps://twitter.com/ndrew_lawrence/status/1093715333079318530 n nSo who is the government entity. Is it an intelligence agency? Saudi? US? Russian?&#39;, &#39;I am really hopeful that this sub doesn &#39;t eat itself alive like it did in 2015/2016. n n*Especially* now that we know just how extensive bad faith accounts are - foreign and domestic. n nIt is perfectly healthy to have discourse. It &#39;s okay to talk about the opinions of the DNC, candidates &#39; negatives and positives, etc.. But that discourse needs to be respectful, constructive, and at least with a sense of **unity** in mind at the end of the day. No one should be shoving support down peoples &#39; throats. n nBe vigilant for these divisive accounts and comments. It &#39;s going to be a hell of a field, and I &#39;m excited to see these candidates lay out their campaigns. n nEdit: To anyone that wasn &#39;t on this sub in 2016, n n[This is what I mean.](https://old.reddit.com/r/politics/comments/4393km/no_reporters_allowed_at_hillarys_wall_st_speeches/) 20,000 upvotes for a dailycaller article, FFS. n n[Oh wow, 26k for another dailycaller article](https://old.reddit.com/r/politics/comments/4durh2/clinton_campaign_uses_noise_machine_to_block/) n n[20k for Fox news](https://old.reddit.com/r/politics/comments/4hwwhm/romanian_hacker_guccifer_i_breached_clinton/) n n[18k for Washingtontimes](https://old.reddit.com/r/politics/comments/4ly7eq/hillary_clinton_yet_to_hold_single_press/) n n[23k Upvotes for Breitbart](https://old.reddit.com/r/politics/comments/4mamz7/hillary_clinton_posted_names_of_hidden/) n n[24k for &#34;SputnikNews&#34;](https://old.reddit.com/r/politics/comments/4m3fd4/snowden_slams_us_for_ignoring_hillary_clintons/) Literally a Russian government-owned news source.&#39;, &#34;Bernie is an upstanding, compassionate person. He has shown he walks the walk standing up for the American people. He has new ideas that can really change our country for the better. I see kindness and experience in him. He&#39;s certainly not too old, and seems to be in great health, and in great spirits! n nI hope he does well, and if not I&#39;ll be voting for the majority Democrat candidate, because as much as I love Bernie, we need to not let 2016 repeat and have votes go away from fighting the corrupt Republicans. n nHe&#39;s not just another old white man, and it&#39;s really sad to see this repeated. He is a progressive in every way, and has done more for non-white people and women than many in politics! n nI would love to see Warren and Bernie together somehow in the end!&#34;, &#39;Genuine Curiosity: How does this work? AOC is not personally paying for staffers. Is she given a total budget for her staff, and is simply evening out the pay more/hiring fewer people than other members of congress?&#39;, &#34;It&#39;s brilliant, he is making all Americans pay for the wall, and will declare another national emergency to declare all of us Mexicans n nEdit: My gold cherry popped, thanks stranger. &#34;, &#34;I attended Fordham University. I&#39;m gonna go donate for the first time since graduation as a thank you for confirming this. &#34;, &#34;So, I&#39;ve been occupied all day, just getting to look at the news right now... n nIt&#39;s safe to say Michael Cohen is going to be dropping bombs tomorrow, right? n nEdit: As in testimony along with supporting documents on multiple crimes that will possibly initiate impeachment proceedings?&#34;, &#34;AOC is a noble martyr. She wears bright colors and wants attention. Not because she&#39;s trying to evade scrutiny, but because scrutiny is exactly what she&#39;s trying to encourage. Then she speaks in plain language in a very rational way. She makes the political process more accessible. n nNo wonder she has so many enemies; she&#39;s shining light in the dark, insidious crevices of our political system. I&#39;m proud of anyone whom stands up for transparency. It&#39;s no easy task.&#34;, &#34;A post I saw on a conservative sub on a post about British citizen having her citizenship revoked because she joined w/ ISIS, someone asked if this was hypocritical because they criticized Obama for Anwar al Alwaki: n n&amp;gt;Meh. I feel like we only cared about this because we could bash Obama with it. Do you truly care that someone collaborating with terrorists was killed without a trial? I don&#39;t. I hope this person gets droned and trump posts a dank meme of it on Twitter. n nThey&#39;re not arguing in good faith it&#39;s a mistake to think they are&#34;, &#39;Just donated $250 n nGo bernie go!&#39;, &#34;How do you ensure UBI doesn&#39;t result in mass inflation? If everyone gets a free $1000 a month, what is to stop the market from jacking up prices?&#34;], dtype=object) . Exploring the results where our model was most wrong is a good way to figure out where we can tweak our model to improve results. Above I queried comments where our model thought it would be a downvoted comment but the comment ended up with 50+ upvotes. Another option would be to query results where the model had high confidence in a particular category but was wrong in its prediction. . . Summary . Keras is a high level API yet it still offers the flexibility to create dynamic deep learning models. Our initial results may not be state of the art but predicting comment score is a very complex problem with countless dependencies that could affect it. . Resources: . https://www.johnwittenauer.net/deep-learning-with-keras-structured-time-series/ | https://www.tensorflow.org/alpha/tutorials/keras/feature_columns | https://realpython.com/python-keras-text-classification/ | https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html | https://github.com/titu1994/keras-one-cycle | https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf | https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1 | https://www.youtube.com/watch?v=FmpDIaiMIeA | https://end-to-end-machine-learning.teachable.com/courses/how-deep-neural-networks-work/lectures/9533961 | https://api.pushshift.io/reddit/search/submission/?ids=9yuili | https://github.com/titu1994/keras-one-cycle | http://colah.github.io/posts/2015-08-Understanding-LSTMs/ | https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models | http://ruder.io/text-classification-tensorflow-estimators/ | https://www.tensorflow.org/guide/datasets#consuming_csv_data | https://medium.freecodecamp.org/how-to-transfer-large-files-to-google-colab-and-remote-jupyter-notebooks-26ca252892fa | https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1 | .",
            "url": "https://gardnmi.github.io/blog/jupyter/pandas/keras/reddit/deep-learning/2020/10/19/reddit-score-prediction.html",
            "relUrl": "/jupyter/pandas/keras/reddit/deep-learning/2020/10/19/reddit-score-prediction.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Not Hotdog App with fast.ai",
            "content": "The code for this project can be found here. . . Imports . from fastai.vision import * from google_images_download import google_images_download . fasai handles all the dependencies for you. You simply call from fastai.vision import * and that is it. I also imported a library called google_images_download which I will use next to download our imageset from google images. . . Data Preparation . . ! googleimagesdownload -cf google_config.json . . For our Not Hotdog app to work we are going to need to train it with images of hotdogs and &quot;not hotdogs&quot;. To get the images we use the library googleimagesdownload . The json file above are parameters for the library. We are essentially telling it to Query Google Images once for &quot;hotdog&quot; and download the first 100 images into a folder called &quot;images/hotdog&quot; and then again for &quot;random pictures&quot; and store them in a folder called &quot;images/not_hotdog&quot;. . We execute the code by calling ! googleimagesdownload -cf google_config.json We now have the images we need for our deep learning model. . path = Path(&#39;images&#39;) data = ImageDataBunch.from_folder(path, train=[&#39;hotdog&#39;, &#39;not_hotdog&#39;], valid_pct=0.2, ds_tfms=get_transforms(), size=224, num_workers=0).normalize(imagenet_stats) . fastai uses a &quot;Learner&quot; object to train a model. In order to establish a learner with your data you first need to create a &quot;ImageDataBunch&quot; object. . Above we: . Set the path for the location of the image folders | Provide the names of the two folders we are labeling | Create a validation set from out images using 20% of them as validation | Transform the Images | Set the size of the Images | Normalize the Data | . data.show_batch(rows=3, figsize=(7,8)) . print( f&#39;Labels: {data.classes} n&#39; f&#39;Training Set Size: {len(data.train_ds)} n&#39; f&#39;Validation Set Size: {len(data.valid_ds)}&#39; ) . Labels: [&#39;hotdog&#39;, &#39;not_hotdog&#39;] Training Set Size: 143 Validation Set Size: 35 . Now that we have our databunch created data we can preview our transformed / normalized images. We can also access the Labels that we are trying to predict as well as the train and validation sizes. . . Training . learn = cnn_learner(data, models.resnet34, metrics=error_rate, callback_fns=ShowGraph) . learn.fit_one_cycle(4) . Total time: 05:25 epoch train_loss valid_loss error_rate time . 0 | 1.066794 | 1.197378 | 0.485714 | 01:23 | . 1 | 0.675965 | 0.504514 | 0.200000 | 01:23 | . 2 | 0.495727 | 0.335483 | 0.142857 | 01:17 | . 3 | 0.387999 | 0.246207 | 0.085714 | 01:19 | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.save(&#39;stage-1&#39;) . To train our model we create a cnn_learner which specializes in image labeling. We pass in the databunch we created earlier data and then use the default arguments found in the documentation. . We then fit the model using learn.fit_one_cycle(4) . As you can see each time the dataset is trained (epoch) the model performance get slightly better. . Finally we save the current iteration of the model just in case. . learn.unfreeze() . learn.lr_find() . LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . learn.recorder.plot() . learn.fit_one_cycle(2, max_lr=slice(1e-05, 1e-04)) . Total time: 03:17 epoch train_loss valid_loss error_rate time . 0 | 0.132134 | 0.168690 | 0.057143 | 01:40 | . 1 | 0.101042 | 0.126670 | 0.057143 | 01:35 | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.save(&#39;stage-2&#39;) . The next step is to optimize our model using the learning rate finder. The learning rate is what tells our model how fast or how slow it should minimize loss. A learning rate to high might never find the minimum loss and too low and it could take too long. . We call learn.lr_find() and then learn.recorder.plot() . According to the documentation we are supposed to pick out a range on the plot that has the steepest slope. I&#39;m still a little unsure about this but I opt for the range of &quot;1e-05 to 1e-04&quot;. . We then fit the model one last time using the new learning rate range. . . Evaluation . . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix() . Now that our model is sufficiently trained let&#39;s check the results of the validation set with a Confusion Matrix. As you can see below we have 2 wrong predictions where our model thought the picture was a &quot;hotdog&quot; when in fact it was a &quot;not_hotdog&quot;. . interp.plot_top_losses(2) . By calling interp.plot_top_losses(2) we can see the two images that were incorrectly labeled. As you can see both images somewhat resemble the shape of a hotdog. . . Prediction . . hotdog = open_image(r&#39;test hotdog.png&#39;) hamburger = open_image(r&#39;test hamburger.jpg&#39;) dog = open_image(r&#39;test puppy.jpg&#39;) . for name, img in zip([&#39;hotdog&#39;, &#39;hamburger&#39;, &#39;dog&#39;], [hotdog, hamburger, dog]): pred_class, _ , _ = learn.predict(img) print(f&#39;{name} is {pred_class}&#39;) . hotdog is hotdog hamburger is hotdog dog is not_hotdog . The last step in the process is using our model to actually predict new images. Above I import 3 new images using open_image . We then use learn.predict on our images which returns the class we are predicting, its index, and the probability. We are only interested in the class so we put an underscore &quot;_&quot; for the other two. . As you can see it accurately identifies the hotdog as a &quot;hotdog&quot; and the dog as a &quot;not_hotdog&quot;. Unfortunately, the hamburger is labeled incorrectly but as you can see it does sort of resemble a hotdog. The next steps would be to continue to iterate on the model by adding more images and tuning parameters. . . Summary . fastai makes it very easy for anyone that wants to be a practitioner of deep learning. It allows the user to create very accurate models that a few years ago would have been considered state of the art. The fastai library is very well documented and has plenty of examples to follow along with. I highly recommend you check it out. . &lt;/div&gt; .",
            "url": "https://gardnmi.github.io/blog/jupyter/pandas/keras/reddit/deep-learning/2020/10/19/not-hotdog.html",
            "relUrl": "/jupyter/pandas/keras/reddit/deep-learning/2020/10/19/not-hotdog.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Pandas Tips and Tricks",
            "content": "I spent an entire month answering pandas related StackOverflow question earning over 1,000 reputation. Below are some of the more useful methods outside the basics which can used to solve common occuring data transformation problems. . . import pandas as pd import numpy as np . np.where . A faster way to implement an if else condition . df = pd.DataFrame([&#39;Dog&#39;, &#39;Cat&#39;] * 2, columns=[&#39;Adult&#39;]) df . Adult . 0 | Dog | . 1 | Cat | . 2 | Dog | . 3 | Cat | . df[&#39;Baby&#39;] = np.where(df[&#39;Adult&#39;].eq(&#39;Dog&#39;), &#39;Puppy&#39;, &#39;Kitten&#39;) df . Adult Baby . 0 | Dog | Puppy | . 1 | Cat | Kitten | . 2 | Dog | Puppy | . 3 | Cat | Kitten | . np.select . An alternative to np.where when you have multiple conditions . df = pd.DataFrame({&quot;color&quot; : [&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;black&quot;, &quot;white&quot;, &quot;purple&quot;]}) df . color . 0 | red | . 1 | green | . 2 | blue | . 3 | black | . 4 | white | . 5 | purple | . conditions = [df.color.isin([&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]), df.color.eq(&#39;black&#39;) | df.color.eq(&#39;white&#39;)] # create a list of choices, if both conditions are True then the first choice is choosen choices = [&#39;rgb&#39;, &#39;b&amp;w&#39;] df[&#39;category&#39;] = np.select(condlist=conditions, choicelist=choices, default=df.color) df . color category . 0 | red | rgb | . 1 | green | rgb | . 2 | blue | rgb | . 3 | black | b&amp;w | . 4 | white | b&amp;w | . 5 | purple | purple | . def func(df): if df.color in [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]: return &#39;rgb&#39; elif df.color == &#39;black&#39; or df.color == &#39;white&#39;: return &#39;b&amp;w&#39; else: return df.color df[&#39;category&#39;] = df.apply(func, axis=1) df . color category . 0 | red | rgb | . 1 | green | rgb | . 2 | blue | rgb | . 3 | black | b&amp;w | . 4 | white | b&amp;w | . 5 | purple | purple | . Mask / Where . A quick way to return a value when a condition is (not) met. . https://stackoverflow.com/questions/51982417/pandas-mask-where-methods-versus-numpy-np-where . df = pd.DataFrame([[&#39;1&#39;, 10], [&#39;1&#39;, 30], [&#39;1&#39;, 10], [&#39;2&#39;, 40], [&#39;2&#39;, 40], [&#39;2&#39;, 40], [&#39;3&#39;, 20], [&#39;3&#39;, 40], [&#39;3&#39;, 10]], columns=(&#39;id&#39;, &#39;sample&#39;)) df . id sample . 0 | 1 | 10 | . 1 | 1 | 30 | . 2 | 1 | 10 | . 3 | 2 | 40 | . 4 | 2 | 40 | . 5 | 2 | 40 | . 6 | 3 | 20 | . 7 | 3 | 40 | . 8 | 3 | 10 | . df.mask(df[&#39;id&#39;].eq(&#39;2&#39;), &#39;mask&#39;) . id sample . 0 | 1 | 10 | . 1 | 1 | 30 | . 2 | 1 | 10 | . 3 | mask | mask | . 4 | mask | mask | . 5 | mask | mask | . 6 | 3 | 20 | . 7 | 3 | 40 | . 8 | 3 | 10 | . df.where(df[&#39;id&#39;].eq(&#39;2&#39;), &#39;where&#39;) . id sample . 0 | where | where | . 1 | where | where | . 2 | where | where | . 3 | 2 | 40 | . 4 | 2 | 40 | . 5 | 2 | 40 | . 6 | where | where | . 7 | where | where | . 8 | where | where | . Align dummy columns between train and test sets . In machine learning, your train and test sets may become misaligned when creating dummy columns. This will result in an error. . https://stackoverflow.com/questions/58136267/check-reference-list-in-pandas-column-using-numpy-vectorization . df = pd.DataFrame([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;Z&#39;, &#39;A&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;X&#39;, &#39;Y&#39;], columns=[&#39;Letter&#39;]) df . Letter . 0 | A | . 1 | B | . 2 | C | . 3 | Z | . 4 | A | . 5 | C | . 6 | D | . 7 | E | . 8 | X | . 9 | Y | . train = pd.get_dummies(df.iloc[:4]) train . Letter_A Letter_B Letter_C Letter_Z . 0 | 1 | 0 | 0 | 0 | . 1 | 0 | 1 | 0 | 0 | . 2 | 0 | 0 | 1 | 0 | . 3 | 0 | 0 | 0 | 1 | . test = pd.get_dummies(df.iloc[4:]) test . Letter_A Letter_C Letter_D Letter_E Letter_X Letter_Y . 4 | 1 | 0 | 0 | 0 | 0 | 0 | . 5 | 0 | 1 | 0 | 0 | 0 | 0 | . 6 | 0 | 0 | 1 | 0 | 0 | 0 | . 7 | 0 | 0 | 0 | 1 | 0 | 0 | . 8 | 0 | 0 | 0 | 0 | 1 | 0 | . 9 | 0 | 0 | 0 | 0 | 0 | 1 | . test = test.reindex(columns=train.columns, fill_value=0) test . Letter_A Letter_B Letter_C Letter_Z . 4 | 1 | 0 | 0 | 0 | . 5 | 0 | 0 | 1 | 0 | . 6 | 0 | 0 | 0 | 0 | . 7 | 0 | 0 | 0 | 0 | . 8 | 0 | 0 | 0 | 0 | . 9 | 0 | 0 | 0 | 0 | . Understanding Groupby . Grouping your data can solve a lot of transformation challenges. It&#39;s a good idea to understand how it works. . https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html . df = pd.DataFrame({ &#39;Name&#39;: [&#39;David&#39;, &#39;David&#39;, &#39;Steve&#39; ,&#39;Harry&#39;, &#39;Harry&#39;], &#39;Zip Code&#39;: [55555, 55555, 44444, 55555, 66666] }) df . Name Zip Code . 0 | David | 55555 | . 1 | David | 55555 | . 2 | Steve | 44444 | . 3 | Harry | 55555 | . 4 | Harry | 66666 | . group = df.groupby(&#39;Name&#39;) . for key, dataframe in group: print(f&#39;{key} n {dataframe} n&#39;) . David Name Zip Code 0 David 55555 1 David 55555 Harry Name Zip Code 3 Harry 55555 4 Harry 66666 Steve Name Zip Code 2 Steve 44444 . group.apply(lambda x: type(x)) . Name David &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Harry &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Steve &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; dtype: object . Groupby Filtering . If you want to select subsets of data within each group you can use filter. . https://stackoverflow.com/questions/58453245/pandas-match-and-unmatch-on-differnt-columns-in-python . df = pd.DataFrame({ &#39;Name&#39;: [&#39;David&#39;, &#39;David&#39;, &#39;Steve&#39; ,&#39;Harry&#39;, &#39;Harry&#39;], &#39;Zip Code&#39;: [55555, 55555, 44444, 55555, 66666] }) df . Name Zip Code . 0 | David | 55555 | . 1 | David | 55555 | . 2 | Steve | 44444 | . 3 | Harry | 55555 | . 4 | Harry | 66666 | . df.groupby(&#39;Name&#39;).nunique() . Name Zip Code . Name . David | 1 | 1 | . Harry | 1 | 2 | . Steve | 1 | 1 | . df.groupby(&#39;Name&#39;).filter(lambda x: x[&#39;Zip Code&#39;].nunique() &gt; 1) # alternative using transform df[(df.groupby(&#39;Name&#39;).transform(&#39;nunique&#39;) &gt; 1).values] . Name Zip Code . 3 | Harry | 55555 | . 4 | Harry | 66666 | . Groupby Transform . Returns an aggregated result of the same size of the original dataframe . df = pd.DataFrame({ &#39;Name&#39;: [&#39;David&#39;, &#39;David&#39;, &#39;Steve&#39; ,&#39;Harry&#39;, &#39;Harry&#39;], &#39;Count&#39;: [10, 15, 20, 25, 5] }) df . Name Count . 0 | David | 10 | . 1 | David | 15 | . 2 | Steve | 20 | . 3 | Harry | 25 | . 4 | Harry | 5 | . df.groupby(&#39;Name&#39;).sum() . Count . Name . David | 25 | . Harry | 30 | . Steve | 20 | . df[&#39;Total&#39;] = df.groupby(&#39;Name&#39;).transform(&#39;sum&#39;) df . Name Count Total . 0 | David | 10 | 25 | . 1 | David | 15 | 25 | . 2 | Steve | 20 | 20 | . 3 | Harry | 25 | 30 | . 4 | Harry | 5 | 30 | . Convert an Uneven Dictonary or List to a Dataframe . https://stackoverflow.com/questions/19736080/creating-dataframe-from-a-dictionary-where-entries-have-different-lengths . d = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) ) d . {&#39;A&#39;: array([1, 2]), &#39;B&#39;: array([1, 2, 3, 4])} . pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in d.items() ])) . A B . 0 | 1.0 | 1 | . 1 | 2.0 | 2 | . 2 | NaN | 3 | . 3 | NaN | 4 | . mylist = [[1,2,3], [4,5], 6] mylist . [[1, 2, 3], [4, 5], 6] . pd.DataFrame([ pd.Series(value) for value in mylist ]) . 0 1 2 . 0 | 1.0 | 2.0 | 3.0 | . 1 | 4.0 | 5.0 | NaN | . 2 | 6.0 | NaN | NaN | . Convert Uneven List of Tuples to a Dataframe . https://stackoverflow.com/questions/58450965/list-of-lists-conversion-to-pandas-dataframe . data = [ [(&#39;category&#39;, &#39;evaluation&#39;), (&#39;polarity&#39;, &#39;pos&#39;), (&#39;strength&#39;, &#39;1&#39;), (&#39;type&#39;, &#39;good&#39;)], [(&#39;category&#39;, &#39;intensifier&#39;), (&#39;type&#39;, &#39;shifter&#39;)], [(&#39;category&#39;, &#39;evaluation&#39;), (&#39;polarity&#39;, &#39;pos&#39;), (&#39;strength&#39;, &#39;2&#39;), (&#39;type&#39;, &#39;good&#39;)] ] . [dict(e) for e in data] . [{&#39;category&#39;: &#39;evaluation&#39;, &#39;polarity&#39;: &#39;pos&#39;, &#39;strength&#39;: &#39;1&#39;, &#39;type&#39;: &#39;good&#39;}, {&#39;category&#39;: &#39;intensifier&#39;, &#39;type&#39;: &#39;shifter&#39;}, {&#39;category&#39;: &#39;evaluation&#39;, &#39;polarity&#39;: &#39;pos&#39;, &#39;strength&#39;: &#39;2&#39;, &#39;type&#39;: &#39;good&#39;}] . df = pd.DataFrame(data=[dict(e) for e in data]) df . category polarity strength type . 0 | evaluation | pos | 1 | good | . 1 | intensifier | NaN | NaN | shifter | . 2 | evaluation | pos | 2 | good | . Decay ffill . A vectorized implementation of a custom forward fill function. . https://stackoverflow.com/questions/58404949/how-to-speed-up-a-decay-ffill-function-for-pandas-series . s = pd.Series([0,0, 10, 15, 0,0,25,0,0,20]) . s . 0 0 1 0 2 10 3 15 4 0 5 0 6 25 7 0 8 0 9 20 dtype: int64 . decay = 0.8 (s.mask(s.eq(0)).ffill() * decay ** s.groupby(s.ne(0).cumsum()).cumcount()).fillna(0) . 0 0.0 1 0.0 2 10.0 3 15.0 4 12.0 5 9.6 6 25.0 7 20.0 8 16.0 9 20.0 dtype: float64 . Create Columns in a Loop . https://stackoverflow.com/questions/58364225/grouping-by-and-applying-lambda-with-condition-for-the-first-row-pandas . df = pd.DataFrame(np.arange(1,6), columns=[&#39;number&#39;]) df . number . 0 | 1 | . 1 | 2 | . 2 | 3 | . 3 | 4 | . 4 | 5 | . for num in df.number: df[f&#39;number x {num}&#39;] = num * df.number df . number number x 1 number x 2 number x 3 number x 4 number x 5 . 0 | 1 | 1 | 2 | 3 | 4 | 5 | . 1 | 2 | 2 | 4 | 6 | 8 | 10 | . 2 | 3 | 3 | 6 | 9 | 12 | 15 | . 3 | 4 | 4 | 8 | 12 | 16 | 20 | . 4 | 5 | 5 | 10 | 15 | 20 | 25 | . Explode . Transform List Like Elements into individual rows . (only v.25 and above) . pd.__version__ . &#39;0.25.1&#39; . df = pd.DataFrame([[[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]], [[&#39;D&#39;, &#39;E&#39;, &#39;F&#39;]]], columns=[&#39;lists&#39;]) df . lists . 0 | [A, B, C] | . 1 | [D, E, F] | . df.explode(&#39;lists&#39;) . lists . 0 | A | . 0 | B | . 0 | C | . 1 | D | . 1 | E | . 1 | F | . Expand Dataframe based on a Date Range . Expand Dataframe to range of Dates and fill values . df = pd.DataFrame({ &#39;Date&#39;: [&#39;2018-01-01&#39;, &#39;2018-01-03&#39;], &#39;Product&#39;: [&#39;A&#39;, &#39;B&#39;] } ) df[&#39;Date&#39;] = pd.to_datetime(df[&#39;Date&#39;]) df . Date Product . 0 | 2018-01-01 | A | . 1 | 2018-01-03 | B | . df = df.set_index(&#39;Date&#39;) df . Product . Date . 2018-01-01 | A | . 2018-01-03 | B | . date_range = pd.date_range(start=df.index.min(), end=pd.DateOffset(days=1) + df.index.max()) date_range . DatetimeIndex([&#39;2018-01-01&#39;, &#39;2018-01-02&#39;, &#39;2018-01-03&#39;, &#39;2018-01-04&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) . df.reindex(date_range).ffill() . Product . 2018-01-01 | A | . 2018-01-02 | A | . 2018-01-03 | B | . 2018-01-04 | B | . Wide to Long . Similar to melt but can handle prefixed column names . https://stackoverflow.com/questions/58135102/pivot-tables-on-python . df = pd.DataFrame( {&#39;date&#39;: [&#39;1/1/11&#39;, &#39;2/1/11&#39;, &#39;3/1/11&#39;], &#39;online_won&#39;: [9, 1, 10], &#39;retail_won&#39;: [10, 2, 8], &#39;outbound_won&#39;: [11, 13, 14], &#39;online_leads&#39;: [12, 15, 17], &#39;retail_leads&#39;: [14.0, np.nan, np.nan], &#39;outbound_leads&#39;: [np.nan, np.nan, np.nan]} ) df . date online_won retail_won outbound_won online_leads retail_leads outbound_leads . 0 | 1/1/11 | 9 | 10 | 11 | 12 | 14.0 | NaN | . 1 | 2/1/11 | 1 | 2 | 13 | 15 | NaN | NaN | . 2 | 3/1/11 | 10 | 8 | 14 | 17 | NaN | NaN | . df.columns = [&#39;_&#39;.join(x.split(&#39;_&#39;)[::-1]) for x in df.columns ] df.columns . Index([&#39;date&#39;, &#39;won_online&#39;, &#39;won_retail&#39;, &#39;won_outbound&#39;, &#39;leads_online&#39;, &#39;leads_retail&#39;, &#39;leads_outbound&#39;], dtype=&#39;object&#39;) . df = pd.wide_to_long(df, [&#39;won&#39;,&#39;leads&#39;], &#39;date&#39;, &#39;source&#39;, sep=&#39;_&#39;, suffix=&#39; w+&#39;) df . won leads . date source . 1/1/11 | online | 9 | 12.0 | . 2/1/11 | online | 1 | 15.0 | . 3/1/11 | online | 10 | 17.0 | . 1/1/11 | retail | 10 | 14.0 | . 2/1/11 | retail | 2 | NaN | . 3/1/11 | retail | 8 | NaN | . 1/1/11 | outbound | 11 | NaN | . 2/1/11 | outbound | 13 | NaN | . 3/1/11 | outbound | 14 | NaN | . Rolling Mean . s = pd.Series(np.arange(1,10)) s . 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 dtype: int32 . s.expanding().mean() . 0 1.0 1 1.5 2 2.0 3 2.5 4 3.0 5 3.5 6 4.0 7 4.5 8 5.0 dtype: float64 . s.cumsum() / (s.index.values + 1) . 0 1.0 1 1.5 2 2.0 3 2.5 4 3.0 5 3.5 6 4.0 7 4.5 8 5.0 dtype: float64 . Rank Multiple Columns . https://stackoverflow.com/questions/58136881/compute-rank-average-for-multiple-columns-manually . df = pd.DataFrame(data={&#39;String&#39;:[&#39;a&#39;,&#39;a&#39;,&#39;a&#39;,&#39;a&#39;,&#39;b&#39;],&#39;Integer&#39;:[1,2,3,3,1]}) df . String Integer . 0 | a | 1 | . 1 | a | 2 | . 2 | a | 3 | . 3 | a | 3 | . 4 | b | 1 | . df = df.sort_values([&#39;String&#39;,&#39;Integer&#39;], ascending=[True, False]) df . String Integer . 2 | a | 3 | . 3 | a | 3 | . 1 | a | 2 | . 0 | a | 1 | . 4 | b | 1 | . df[&#39;rank&#39;] = np.arange(len(df)) + 1 df . String Integer rank . 2 | a | 3 | 1 | . 3 | a | 3 | 2 | . 1 | a | 2 | 3 | . 0 | a | 1 | 4 | . 4 | b | 1 | 5 | . df[&#39;rank&#39;] = df.groupby([&#39;String&#39;, &#39;Integer&#39;])[&#39;rank&#39;].transform(&#39;mean&#39;) df . String Integer rank . 2 | a | 3 | 1.5 | . 3 | a | 3 | 1.5 | . 1 | a | 2 | 3.0 | . 0 | a | 1 | 4.0 | . 4 | b | 1 | 5.0 | . df[&#39;rank&#39;] = df.groupby([&#39;String&#39;, &#39;Integer&#39;], sort=False).ngroup().rank() df . String Integer rank . 2 | a | 3 | 1.5 | . 3 | a | 3 | 1.5 | . 1 | a | 2 | 3.0 | . 0 | a | 1 | 4.0 | . 4 | b | 1 | 5.0 | . Select Dates Ignoring Year . Slice Dataframe between two dates ignoring years . https://stackoverflow.com/questions/58117659/select-rows-in-pandas-dataframe-between-dates-regardless-of-year . df = pd.DataFrame({&#39;date&#39;: pd.date_range(&#39;2016-09-01&#39;,&#39;2019-09-01&#39;, freq=&#39;D&#39;)}) df.head() . date . 0 | 2016-09-01 | . 1 | 2016-09-02 | . 2 | 2016-09-03 | . 3 | 2016-09-04 | . 4 | 2016-09-05 | . s = df.date.dt.strftime(&#39;%m%d&#39;).astype(int) s.head() . 0 901 1 902 2 903 3 904 4 905 Name: date, dtype: int32 . df[s.between(316,318)] . date . 196 | 2017-03-16 | . 197 | 2017-03-17 | . 198 | 2017-03-18 | . 561 | 2018-03-16 | . 562 | 2018-03-17 | . 563 | 2018-03-18 | . 926 | 2019-03-16 | . 927 | 2019-03-17 | . 928 | 2019-03-18 | . Split a Dataframe np.split . df = pd.DataFrame({&#39;A&#39;:np.arange(10), &#39;B&#39;:np.arange(10)}) df . A B . 0 | 0 | 0 | . 1 | 1 | 1 | . 2 | 2 | 2 | . 3 | 3 | 3 | . 4 | 4 | 4 | . 5 | 5 | 5 | . 6 | 6 | 6 | . 7 | 7 | 7 | . 8 | 8 | 8 | . 9 | 9 | 9 | . df1, df2 = np.split(df, 2) . df1 . A B . 0 | 0 | 0 | . 1 | 1 | 1 | . 2 | 2 | 2 | . 3 | 3 | 3 | . 4 | 4 | 4 | . df2 . A B . 5 | 5 | 5 | . 6 | 6 | 6 | . 7 | 7 | 7 | . 8 | 8 | 8 | . 9 | 9 | 9 | . pd.concat([df1,df2.reset_index(drop=True)], axis=1) . A B A B . 0 | 0 | 0 | 5 | 5 | . 1 | 1 | 1 | 6 | 6 | . 2 | 2 | 2 | 7 | 7 | . 3 | 3 | 3 | 8 | 8 | . 4 | 4 | 4 | 9 | 9 | . Apply function with multiple arguments . https://stackoverflow.com/questions/58089770/using-apply-function-dataframe . import math x = [-0.75853, -0.75853, -0.75853, -0.75852] y = [-0.63435, -0.63434, -0.63435, -0.63436] z = [-0.10488, -0.10490, -0.10492, -0.10495] w = [-0.10597, -0.10597, -0.10597, -0.10596] df = pd.DataFrame([x, y, z, w], columns=[&#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;w&#39;]) df . x y z w . 0 | -0.75853 | -0.75853 | -0.75853 | -0.75852 | . 1 | -0.63435 | -0.63434 | -0.63435 | -0.63436 | . 2 | -0.10488 | -0.10490 | -0.10492 | -0.10495 | . 3 | -0.10597 | -0.10597 | -0.10597 | -0.10596 | . def roll(qw, qx, qy, qz, var=10): # x-axis rotation sinr_cosp = +2.0 * (qw * qx + qy + qz) cosr_cosp = +1.0 - 2.0 * (qx * qx + qy * qy) roll = math.atan2(sinr_cosp, cosr_cosp) return roll * var df.apply(lambda x : roll(x[&#39;w&#39;], x[&#39;x&#39;], x[&#39;y&#39;], x[&#39;z&#39;], 20), axis=1) . 0 -43.509430 1 -38.182063 2 -7.883262 3 -7.957697 dtype: float64 . def roll(df, var=10): # x-axis rotation sinr_cosp = +2.0 * (df.w * df.x + df.y + df.z) cosr_cosp = +1.0 - 2.0 * (df.x * df.x + df.y * df.y) roll = math.atan2(sinr_cosp, cosr_cosp) return roll * var df.apply(roll, args=(2,), axis=1) . 0 -4.350943 1 -3.818206 2 -0.788326 3 -0.795770 dtype: float64 . Custom Styling . https://stackoverflow.com/questions/58087857/applying-pandas-styles-to-arbitrary-non-product-subsets-of-a-dataframe . df = pd.DataFrame(data={&#39;A&#39;: [0, 1, np.nan], &#39;B&#39;: [.5, np.nan, 0], &#39;C&#39;: [np.nan, 1, 1]}) df . A B C . 0 | 0.0 | 0.5 | NaN | . 1 | 1.0 | NaN | 1.0 | . 2 | NaN | 0.0 | 1.0 | . from matplotlib.cm import get_cmap cmap = get_cmap(&#39;PuBu&#39;) # update with low-high option def threshold(x,low=0,high=1,mid=0.5): # nan cell if np.isnan(x): return &#39;&#39; # non-nan cell x = (x-low)/(high-low) background = f&#39;background-color: rgba{cmap (x, bytes=True)}&#39; text_color = f&#39;color: white&#39; if x &gt; mid else &#39;&#39; return background+&#39;;&#39;+text_color # apply the style df.style.applymap(threshold, low=-1, high=1, mid=0.3) . A B C . 0 0 | 0.5 | nan | . 1 1 | nan | 1 | . 2 nan | 0 | 1 | . Reverse an Array . https://stackoverflow.com/questions/6771428/most-efficient-way-to-reverse-a-numpy-array . np.arange(1,11) . array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) . np.arange(1,11)[::-1] . array([10, 9, 8, 7, 6, 5, 4, 3, 2, 1]) . X = np.arange(1,11).reshape(2,5) X . array([[ 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10]]) . np.fliplr(X) . array([[ 5, 4, 3, 2, 1], [10, 9, 8, 7, 6]]) . Symmetric difference . Values not common to both . https://stackoverflow.com/questions/45845965/python-compare-two-columns-of-features-return-values-which-are-not-common-to . df1 = pd.DataFrame({&#39;a&#39;:[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;x&#39;, &#39;d&#39;, &#39;l&#39;, &#39;z&#39;]}) df1 . a . 0 | a | . 1 | b | . 2 | c | . 3 | x | . 4 | d | . 5 | l | . 6 | z | . df2 = pd.DataFrame({&#39;b&#39;: [&#39;b&#39;, &#39;a&#39;, &#39;d&#39;, &#39;c&#39;, &#39;y&#39;]}) df2 . b . 0 | b | . 1 | a | . 2 | d | . 3 | c | . 4 | y | . pd.Index.symmetric_difference(pd.Index(df1.a), pd.Index(df2.b)).values . array([&#39;l&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;], dtype=object) . np.setdiff1d(np.union1d(df1.a, df2.b), np.intersect1d(df1.a, df2.b)) . array([&#39;l&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;], dtype=object) . Transpose a 3D array . https://stackoverflow.com/questions/32034237/how-does-numpys-transpose-method-permute-the-axes-of-an-array . two_d = np.arange(16).reshape(8,2) print(two_d.shape) print(two_d.transpose().shape) . (8, 2) (2, 8) . three_d = np.arange(20).reshape((1, 4, 5)) print(three_d.shape) print(three_d.transpose(1,0,2).shape) print(three_d.transpose(2,1,0).shape) print(three_d.transpose(0,1,2).shape) . (1, 4, 5) (4, 1, 5) (5, 4, 1) (1, 4, 5) . literal eval . Convert a string representation of a list into a list . https://stackoverflow.com/questions/23111990/pandas-dataframe-stored-list-as-string-how-to-convert-back-to-list . from ast import literal_eval df = pd.DataFrame({&#39;list&#39;:[&#39;[1,2,3]&#39;, &#39;[5,6,7]&#39;]}) df . list . 0 | [1,2,3] | . 1 | [5,6,7] | . df.list.map(lambda x: type(x)) . 0 &lt;class &#39;str&#39;&gt; 1 &lt;class &#39;str&#39;&gt; Name: list, dtype: object . df.list.map(lambda x: type(literal_eval(x))) . 0 &lt;class &#39;list&#39;&gt; 1 &lt;class &#39;list&#39;&gt; Name: list, dtype: object . Difference between Nan and None . https://stackoverflow.com/questions/17534106/what-is-the-difference-between-nan-and-none . str accessor . s = pd.Series([&#39;ABC&#39;,&#39;DEF&#39;]) s . 0 ABC 1 DEF dtype: object . s.str.contains(&#39;A&#39;) . 0 True 1 False dtype: bool . s.str.contains(&#39;A|E&#39;) # A or E . 0 True 1 True dtype: bool . s.str[::-1] . 0 CBA 1 FED dtype: object . s.str[:2] . 0 AB 1 DE dtype: object . Regular Expression . https://stackoverflow.com/questions/4736/learning-regular-expressions . Cartesian Product . df_A = pd.DataFrame({&#39;A&#39;:[1,2,3]}) df_B = pd.DataFrame({&#39;B&#39;:[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;]}) . df_A . A . 0 | 1 | . 1 | 2 | . 2 | 3 | . df_B . B . 0 | A | . 1 | B | . 2 | C | . df_A.assign(key=1).merge(df_B.assign(key=1), on=&#39;key&#39;).drop(&#39;key&#39;, axis=1) . A B . 0 | 1 | A | . 1 | 1 | B | . 2 | 1 | C | . 3 | 2 | A | . 4 | 2 | B | . 5 | 2 | C | . 6 | 3 | A | . 7 | 3 | B | . 8 | 3 | C | . to_frame . Maintain a Dataframe Object . df = pd.DataFrame({&#39;A&#39;:[1,2,3]}) df . A . 0 | 1 | . 1 | 2 | . 2 | 3 | . df.A . 0 1 1 2 2 3 Name: A, dtype: int64 . print(type(df.A)) . &lt;class &#39;pandas.core.series.Series&#39;&gt; . df.A.to_frame() . A . 0 | 1 | . 1 | 2 | . 2 | 3 | . type(df.A.to_frame()) . pandas.core.frame.DataFrame . Date Manipulation . dates = pd.Series(pd.to_datetime([&#39;1/1/2016&#39;, &#39;12/15/2017&#39;, &#39;10/31/2019&#39;])) dates . 0 2016-01-01 1 2017-12-15 2 2019-10-31 dtype: datetime64[ns] . dates + pd.DateOffset(day=1) . C: ProgramData Anaconda3 lib site-packages pandas core arrays datetimes.py:837: PerformanceWarning: Non-vectorized DateOffset being applied to Series or DatetimeIndex PerformanceWarning, . 0 2016-01-01 1 2017-12-01 2 2019-10-01 dtype: datetime64[ns] . dates + pd.offsets.MonthEnd(0) . 0 2016-01-31 1 2017-12-31 2 2019-10-31 dtype: datetime64[ns] . dates.dt.month . 0 1 1 12 2 10 dtype: int64 . dates.dt.day_name() . 0 Friday 1 Friday 2 Thursday dtype: object . Total Seconds . Calculate total seconds between to datetimes . (pd.to_datetime(&#39;1/5/2019 00:01:35&#39;) - pd.to_datetime(&#39;1/1/2019 00:05:00&#39;)).seconds . 86195 . (pd.to_datetime(&#39;1/5/2019 00:01:35&#39;) - pd.to_datetime(&#39;1/1/2019 00:05:00&#39;)).total_seconds() . 345395.0 .",
            "url": "https://gardnmi.github.io/blog/jupyter/pandas/2020/10/18/stack-overflow-tips-and-tricks.html",
            "relUrl": "/jupyter/pandas/2020/10/18/stack-overflow-tips-and-tricks.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Identifying Cross Selling Opportunities with Collaborative Filtering",
            "content": "import numpy as np import pandas as pd from collections import defaultdict import math import seaborn as sns import matplotlib.pyplot as plt from surprise import KNNBasic from surprise import Dataset from surprise import Reader from surprise.model_selection import cross_validate %matplotlib inline . The data is a matrix of purchase history that contains 1,000 different users and 100 different products. If the user owns the product then it will be represented as the number &quot;1&quot; else it will be represented as the number &quot;0&quot;. . Our goal is to have our algorithm recommend products that make sense to cross sell. In order make sure our algorithm works, we&#39;ve created a pattern in the data where the first 100 users only own products 0 thru 9 and the next 100 users own only products 10 thru 19, and so on. There is only a 35% chance that the user will own the product to ensure we have cross selling opportunities. . The below heatmap you can see a visualization of the data. The rows are our 1,000 users and the columns are 100 products. Each green dash is a product owned by that particular user. Each square block is a cluster of products and users. You could think of these clusters as being specific product division within a company. . If our algorithm is working correctly then it should only recommend cross selling opportunities within each user/product cluster. . data = np.zeros((1000, 100)) purchases = np.random.choice(2, size=(1000, 10), p=[.65, .35]) for n in range(1000): # https://stackoverflow.com/questions/26454649/python-round-up-to-the-nearest-ten index = int(math.ceil((n+1) / 100.0)) * 10 data[n, index-10 : index] = purchases[n] df = pd.DataFrame(data) plt.figure(figsize=(13, 10)) sns.heatmap(df, cbar=False, xticklabels=False, yticklabels=False, cmap=sns.color_palette([&quot;#34495e&quot;, &quot;#2ecc71&quot;])); . Of course with the data above you wouldn&#39;t need an algorithm to help you provide recommendations for cross selling but if we were to shuffle the rows and columns around a bit you will get a more sporadic looking dataset that visually looks random. However, the pattern we created still exists within the data even though it&#39;s shuffled. This is where a recommendation algorithm can be useful as it will find any trends if they exist and provide us with useful recommendations. . df = df.sample(frac=1, axis=0).sample(frac=1, axis=1) plt.figure(figsize=(13, 10)) sns.heatmap(df, cbar=False, xticklabels=False, yticklabels=False, cmap=sns.color_palette([&quot;#34495e&quot;, &quot;#2ecc71&quot;])); . The algorithm we are using requires the data be in a transactional format where each row represents the user, the product, and whether or not the product is purchased (1 = purchased, 0 = not purchased). . To better distinguish users from products we&#39;ll prefix the userid with &#39;u&#39; and the productid with &#39;p&#39;. This will be useful in understanding the outputs from our algorithm as it can also similar users if we desire. . Below is the first 10 rows in our data. . df = pd.melt(df.reset_index(), id_vars=[&#39;index&#39;]) df = df.sample(frac=1, axis=0).reset_index(drop=True) df.columns = [&#39;user_id&#39;, &#39;product_id&#39;, &#39;is_purchase&#39;] df.user_id = df.user_id.map(lambda x: f&#39;u_{x}&#39;) df.product_id = df.product_id.map(lambda x: f&#39;p_{x}&#39;) df.head(10) . user_id product_id is_purchase . 0 u_913 | p_99 | 0.0 | . 1 u_521 | p_15 | 0.0 | . 2 u_597 | p_75 | 0.0 | . 3 u_436 | p_78 | 0.0 | . 4 u_799 | p_23 | 0.0 | . 5 u_687 | p_34 | 0.0 | . 6 u_862 | p_87 | 0.0 | . 7 u_520 | p_52 | 0.0 | . 8 u_480 | p_63 | 0.0 | . 9 u_500 | p_7 | 0.0 | . Now that our data is ready we can train/fit the algorithm. The algorithm &quot;KNNBasic&quot; computes a similarity matrix between users and then uses that matrix to predict whether or not the user owns the product. . We&#39;ve created a function called &quot;get_top_n&quot; that by default uses our trained algorithm to make a prediction for every user and then returns the 10 products that had the highest probability of being owned by the user. . If our algorithm is working correctly it will recommend for users 0–99 products 0–9, for users 100–199 products 10–19 and so on. . reader = Reader(rating_scale=(0, 1)) data = Dataset.load_from_df(df, reader) algo = KNNBasic(sim_options={&#39;user_based&#39;: True}) trainset = data.build_full_trainset() algo.fit(trainset) testset = trainset.build_testset() predictions = algo.test(testset) def get_top_n(predictions, n=10): &#39;&#39;&#39; Return the top-N recommendation for each user from a set of predictions. Source: https://surprise.readthedocs.io/en/stable/FAQ.html Args: predictions(list of Prediction objects): The list of predictions, as returned by the test method of an algorithm. n(int): The number of recommendation to output for each user. Default is 10. Returns: A dict where keys are user (raw) ids and values are lists of tuples: [(raw item id, rating estimation), ...] of size n. &#39;&#39;&#39; # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n top_n = get_top_n(predictions, n=10) print(&#39; n First 20 Results: n&#39;) count = 0 for uid, user_ratings in top_n.items(): print(uid, [iid for (iid, _) in user_ratings]) count += 1 if count == 20: break . Computing the msd similarity matrix... Done computing similarity matrix. First 20 Results: u_913 [&#39;p_95&#39;, &#39;p_97&#39;, &#39;p_92&#39;, &#39;p_90&#39;, &#39;p_98&#39;, &#39;p_96&#39;, &#39;p_99&#39;, &#39;p_91&#39;, &#39;p_93&#39;, &#39;p_94&#39;] u_521 [&#39;p_52&#39;, &#39;p_55&#39;, &#39;p_51&#39;, &#39;p_50&#39;, &#39;p_53&#39;, &#39;p_58&#39;, &#39;p_57&#39;, &#39;p_54&#39;, &#39;p_47&#39;, &#39;p_90&#39;] u_597 [&#39;p_53&#39;, &#39;p_59&#39;, &#39;p_52&#39;, &#39;p_57&#39;, &#39;p_54&#39;, &#39;p_56&#39;, &#39;p_50&#39;, &#39;p_51&#39;, &#39;p_58&#39;, &#39;p_55&#39;] u_436 [&#39;p_44&#39;, &#39;p_41&#39;, &#39;p_43&#39;, &#39;p_45&#39;, &#39;p_47&#39;, &#39;p_48&#39;, &#39;p_40&#39;, &#39;p_42&#39;, &#39;p_49&#39;, &#39;p_46&#39;] u_799 [&#39;p_76&#39;, &#39;p_70&#39;, &#39;p_74&#39;, &#39;p_71&#39;, &#39;p_73&#39;, &#39;p_23&#39;, &#39;p_79&#39;, &#39;p_90&#39;, &#39;p_47&#39;, &#39;p_19&#39;] u_687 [&#39;p_61&#39;, &#39;p_60&#39;, &#39;p_63&#39;, &#39;p_64&#39;, &#39;p_66&#39;, &#39;p_69&#39;, &#39;p_68&#39;, &#39;p_62&#39;, &#39;p_65&#39;, &#39;p_67&#39;] u_862 [&#39;p_88&#39;, &#39;p_81&#39;, &#39;p_86&#39;, &#39;p_89&#39;, &#39;p_83&#39;, &#39;p_84&#39;, &#39;p_85&#39;, &#39;p_87&#39;, &#39;p_82&#39;, &#39;p_80&#39;] u_520 [&#39;p_50&#39;, &#39;p_57&#39;, &#39;p_56&#39;, &#39;p_51&#39;, &#39;p_55&#39;, &#39;p_52&#39;, &#39;p_59&#39;, &#39;p_68&#39;, &#39;p_84&#39;, &#39;p_2&#39;] u_480 [&#39;p_44&#39;, &#39;p_49&#39;, &#39;p_45&#39;, &#39;p_43&#39;, &#39;p_47&#39;, &#39;p_42&#39;, &#39;p_40&#39;, &#39;p_41&#39;, &#39;p_48&#39;, &#39;p_46&#39;] u_500 [&#39;p_58&#39;, &#39;p_55&#39;, &#39;p_57&#39;, &#39;p_56&#39;, &#39;p_52&#39;, &#39;p_54&#39;, &#39;p_50&#39;, &#39;p_51&#39;, &#39;p_53&#39;, &#39;p_59&#39;] u_85 [&#39;p_9&#39;, &#39;p_0&#39;, &#39;p_8&#39;, &#39;p_1&#39;, &#39;p_4&#39;, &#39;p_2&#39;, &#39;p_5&#39;, &#39;p_3&#39;, &#39;p_7&#39;, &#39;p_6&#39;] u_706 [&#39;p_76&#39;, &#39;p_75&#39;, &#39;p_78&#39;, &#39;p_70&#39;, &#39;p_72&#39;, &#39;p_71&#39;, &#39;p_79&#39;, &#39;p_74&#39;, &#39;p_77&#39;, &#39;p_73&#39;] u_125 [&#39;p_15&#39;, &#39;p_11&#39;, &#39;p_17&#39;, &#39;p_16&#39;, &#39;p_10&#39;, &#39;p_14&#39;, &#39;p_13&#39;, &#39;p_12&#39;, &#39;p_19&#39;, &#39;p_18&#39;] u_42 [&#39;p_8&#39;, &#39;p_9&#39;, &#39;p_3&#39;, &#39;p_6&#39;, &#39;p_4&#39;, &#39;p_7&#39;, &#39;p_0&#39;, &#39;p_1&#39;, &#39;p_2&#39;, &#39;p_5&#39;] u_958 [&#39;p_99&#39;, &#39;p_94&#39;, &#39;p_91&#39;, &#39;p_96&#39;, &#39;p_92&#39;, &#39;p_93&#39;, &#39;p_95&#39;, &#39;p_98&#39;, &#39;p_97&#39;, &#39;p_90&#39;] u_590 [&#39;p_56&#39;, &#39;p_58&#39;, &#39;p_54&#39;, &#39;p_57&#39;, &#39;p_52&#39;, &#39;p_55&#39;, &#39;p_51&#39;, &#39;p_59&#39;, &#39;p_50&#39;, &#39;p_53&#39;] u_860 [&#39;p_86&#39;, &#39;p_82&#39;, &#39;p_83&#39;, &#39;p_80&#39;, &#39;p_84&#39;, &#39;p_87&#39;, &#39;p_81&#39;, &#39;p_68&#39;, &#39;p_19&#39;, &#39;p_24&#39;] u_823 [&#39;p_88&#39;, &#39;p_85&#39;, &#39;p_84&#39;, &#39;p_83&#39;, &#39;p_87&#39;, &#39;p_86&#39;, &#39;p_23&#39;, &#39;p_76&#39;, &#39;p_79&#39;, &#39;p_89&#39;] u_31 [&#39;p_8&#39;, &#39;p_0&#39;, &#39;p_2&#39;, &#39;p_1&#39;, &#39;p_6&#39;, &#39;p_5&#39;, &#39;p_9&#39;, &#39;p_7&#39;, &#39;p_4&#39;, &#39;p_3&#39;] u_424 [&#39;p_49&#39;, &#39;p_40&#39;, &#39;p_48&#39;, &#39;p_46&#39;, &#39;p_41&#39;, &#39;p_44&#39;, &#39;p_43&#39;, &#39;p_42&#39;, &#39;p_47&#39;, &#39;p_45&#39;] . With the top 10 product recommendations for each user available to us, we can use it to generate Opportunities for specific users. . Below we iterate through the top recommendations for User &#39;u_9&#39; and identify cross selling opportunities with the label &quot;Opportunity:&quot;. . for item in top_n[&#39;u_9&#39;]: if item[0] in df[(df.user_id ==&#39;u_9&#39;) &amp; (df.is_purchase==1)].product_id.to_list(): print(f&#39;Purchased: {item[0]}&#39;) else: print(f&#39;Opporutnity: {item[0]}&#39;) . Purchased: p_9 Purchased: p_4 Purchased: p_1 Opporutnity: p_2 Opporutnity: p_7 Opporutnity: p_3 Opporutnity: p_8 Opporutnity: p_0 Opporutnity: p_5 Opporutnity: p_6 . Because our algorithm computes a similarity matrix we can leverage it to find the n nearest neighbors for any user. This can be leveraged for other tasks such as pricing guidance or would could just retrain our algorithm to predict pricing instead of purchases and use those predictions as pricing recommendations. . Below we show the 25 nearest neighbors to user &quot;u_1&quot;. If it is working correctly we should see mostly 25 users within the range of users 0–99. . neighbors = algo.get_neighbors(trainset.to_inner_uid(&#39;u_1&#39;), k=25) print(&quot;25 most similar Users to &#39;u_1&#39;: n&quot;) print([trainset.to_raw_uid(user) for user in neighbors]) . 25 most similar Users to &#39;u_1&#39;: [&#39;u_30&#39;, &#39;u_73&#39;, &#39;u_23&#39;, &#39;u_76&#39;, &#39;u_51&#39;, &#39;u_86&#39;, &#39;u_36&#39;, &#39;u_78&#39;, &#39;u_35&#39;, &#39;u_68&#39;, &#39;u_26&#39;, &#39;u_63&#39;, &#39;u_11&#39;, &#39;u_2&#39;, &#39;u_32&#39;, &#39;u_85&#39;, &#39;u_31&#39;, &#39;u_54&#39;, &#39;u_67&#39;, &#39;u_61&#39;, &#39;u_7&#39;, &#39;u_45&#39;, &#39;u_21&#39;, &#39;u_66&#39;, &#39;u_25&#39;] . Since we&#39;re using &quot;user based similarities&quot; to provide recommendations we can&#39;t provide recommendations for a new user. This is termed the &quot;cold start problem&quot;. . Instead, we can train a separate algorithm that uses &quot;item based similarities&quot; to recommend products that are similar to one another. For example, if our new user is interested in product 25 &quot;p_25&quot; our algorithm should be able to recommend products 20 thru 29 as cross selling opportunities. . algo = KNNBasic(sim_options={&#39;user_based&#39;: False}) algo.fit(trainset) neighbors = algo.get_neighbors(trainset.to_inner_iid(&#39;p_25&#39;), k=9) print(&quot; n 9 most similar Products to p_25: n&quot;) print([trainset.to_raw_iid(item) for item in neighbors]) . Computing the msd similarity matrix... Done computing similarity matrix. 9 most similar Products to p_25: [&#39;p_21&#39;, &#39;p_29&#39;, &#39;p_28&#39;, &#39;p_22&#39;, &#39;p_26&#39;, &#39;p_27&#39;, &#39;p_23&#39;, &#39;p_24&#39;, &#39;p_20&#39;] . Resources: . http://surpriselib.com/ | https://www.analyticsvidhya.com/blog/2015/10/recommendation-engines/ | https://www.datacamp.com/community/tutorials/recommender-systems-python | https://realpython.com/build-recommendation-engine-collaborative-filtering/ | .",
            "url": "https://gardnmi.github.io/blog/jupyter/pandas/scikit/machine-learning/collaborative-filtering/2020/10/18/identifying-cross-selling-opportunities.html",
            "relUrl": "/jupyter/pandas/scikit/machine-learning/collaborative-filtering/2020/10/18/identifying-cross-selling-opportunities.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "PGA Driving Distance Over the Years",
            "content": "import pandas as pd import altair as alt . . tables = pd.read_html(&#39;https://www.pgatour.com/stats/stat.101.2020.html&#39;) drives = tables[1] drives.head(10) . RANK THIS WEEK RANK LAST WEEK PLAYER NAME ROUNDS AVG. TOTAL DISTANCE TOTAL DRIVES . 0 1 | 1 | Bryson DeChambeau | 8 | 344.4 | 5511 | 16 | . 1 2 | 5 | Dustin Johnson | 4 | 333.8 | 2670 | 8 | . 2 3 | 11 | Rory McIlroy | 8 | 333.4 | 5334 | 16 | . 3 4 | 4 | Joaquin Niemann | 12 | 333.2 | 7996 | 24 | . 4 5 | 6 | Taylor Pendrith | 4 | 331.9 | 2655 | 8 | . 5 6 | 2 | Cameron Champ | 8 | 331.4 | 5302 | 16 | . 6 7 | 7 | Jon Rahm | 8 | 324.8 | 5196 | 16 | . 7 8 | 16 | Tony Finau | 4 | 324.6 | 2597 | 8 | . 8 9 | 17 | Wyndham Clark | 10 | 324.5 | 6489 | 20 | . 9 10 | 9 | Ryan Palmer | 10 | 321.8 | 6436 | 20 | . drives.describe()[&#39;AVG.&#39;] . count 264.000000 mean 301.050000 std 11.742452 min 267.400000 25% 294.100000 50% 301.100000 75% 307.900000 max 344.400000 Name: AVG., dtype: float64 . Average Driving Distance Since 1980 . # Create a List of Years years = pd.date_range(start=&#39;1/1/1980&#39;, end=pd.to_datetime(&#39;today&#39;) + pd.offsets.DateOffset(month=12, day=31), freq=&#39;Y&#39;).year # Loop Through URL&#39;s and create a Dataframe d = [] for year in years: tables = pd.read_html(f&#39;https://www.pgatour.com/stats/stat.101.y{year}.html&#39;) try: drives = tables[1].loc[:100] d.append({&#39;Year&#39;: year, &#39;Average Drive&#39;: drives[&#39;AVG.&#39;].mean()}) except: pass df = pd.DataFrame(d) df[&#39;Average Drive&#39;] = df[&#39;Average Drive&#39;].astype(int) . . # Create a selection that chooses the nearest point &amp; selects based on x-value nearest = alt.selection(type=&#39;single&#39;, nearest=True, on=&#39;mouseover&#39;, fields=[&#39;Year&#39;], empty=&#39;none&#39;) # The basic line line = alt.Chart(df).mark_line(interpolate=&#39;basis&#39;).encode( alt.X(&#39;Year:N&#39;), alt.Y(&#39;Average Drive:Q&#39;, scale=alt.Scale(zero=False)), ) # Transparent selectors across the chart. This is what tells us # the x-value of the cursor selectors = alt.Chart(df).mark_point().encode( x=&#39;Year:N&#39;, opacity=alt.value(0), ).add_selection( nearest ) # Draw points on the line, and highlight based on selection points = line.mark_point().encode( opacity=alt.condition(nearest, alt.value(1), alt.value(0)) ) # Draw text labels near the points, and highlight based on selection text = line.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=alt.condition(nearest, &#39;Average Drive:Q&#39;, alt.value(&#39; &#39;)) ) # Draw a rule at the location of the selection rules = alt.Chart(df).mark_rule(color=&#39;gray&#39;).encode( x=&#39;Year:N&#39;, ).transform_filter( nearest ) # Put the five layers into a chart and bind the data alt.layer( line, selectors, points, rules, text ).properties( width=850, height=500 ) . .",
            "url": "https://gardnmi.github.io/blog/jupyter/pandas/pga/2020/10/18/analyzing-pga-driving-distance-with-pandas.html",
            "relUrl": "/jupyter/pandas/pga/2020/10/18/analyzing-pga-driving-distance-with-pandas.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://gardnmi.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gardnmi.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gardnmi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gardnmi.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}